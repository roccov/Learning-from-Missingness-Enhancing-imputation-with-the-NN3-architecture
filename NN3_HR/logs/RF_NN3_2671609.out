Job ID: 2671609
Start time: sam 14 jun 2025 13:15:09 CEST
GPU: Tesla V100-PCIE-32GB
Starting pipeline...
================================================================================
SIMPLIFIED DEEP LEARNING PIPELINE - PYTORCH ENSEMBLE ONLY
================================================================================

1. LOADING DATA...
✓ Loaded 1,685,354 rows, 470 columns

2. CONFIGURATION:
✓ Date range: 1961-07-31 to 2024-12-31
✓ Target column: Excess_ret_target
✓ Train/Val/Test split: 20/5/1 years
✓ Feature selection: Random Forest (20-50 features)
✓ Model: PyTorch Ensemble (1 models)
✓ Architecture: [32, 16, 8]
✓ Hyperparameter tuning: Grid Search
✓ Grid search parameters: {'l1_lambda': [1e-06, 1e-05], 'weight_decay': [1e-06, 1e-05]}

3. INITIALIZING PIPELINE...
Using device: cuda
Fixed architecture: [32, 16, 8]

4. STARTING SLIDING WINDOW ANALYSIS...
------------------------------------------------------------
Excluded 234 mask/flag features
Using 234 features (down from 470 total columns)
Period: 1961-07 to 2024-12
Training window: 20 years
Validation window: 5 years
Test window: 1 years

=== Window 1 ===
Train: 1961-07 to 1980-12 (69382 samples)
Val:   1981-01 to 1985-12 (41455 samples)
Test:  1986-01 to 1986-12 (10363 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (69382, 234)
Data cleaned successfully
→ Using min_samples_split=693, min_samples_leaf=346 for 69382 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 55505 training samples, 13877 validation samples
  20 features: Time-based R² = -0.0166
  25 features: Time-based R² = -0.0760
  30 features: Time-based R² = -0.0882
  35 features: Time-based R² = -0.1052
  40 features: Time-based R² = -0.1112
  45 features: Time-based R² = -0.1293
  50 features: Time-based R² = -0.1023
Optimal number of features: 20 (Time-based R² = -0.0166)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010527
  *** New best score: 0.010527

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010409
  *** New best score: 0.010409

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010530

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010483

Grid search completed!
Best validation MSE: 0.010409
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0043 | R² TEST: -0.0442 ***
RMSE: 0.115546 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_01_training.png
  └─ Overfitting status: Minimal (Gap: 6.3%)

=== Window 2 ===
Train: 1962-01 to 1981-12 (75879 samples)
Val:   1982-01 to 1986-12 (45295 samples)
Test:  1987-01 to 1987-12 (11553 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (75879, 234)
Data cleaned successfully
→ Using min_samples_split=758, min_samples_leaf=379 for 75879 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 60703 training samples, 15176 validation samples
  20 features: Time-based R² = -0.0324
  25 features: Time-based R² = -0.0564
  30 features: Time-based R² = -0.0058
  35 features: Time-based R² = -0.0054
  40 features: Time-based R² = 0.0032
  45 features: Time-based R² = 0.0201
  50 features: Time-based R² = 0.0220
Optimal number of features: 50 (Time-based R² = 0.0220)
Selected 50 optimal features from Random Forest
Feature reduction: 234 → 50 (21.4%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011135
  *** New best score: 0.011135

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011228

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011450

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011172

Grid search completed!
Best validation MSE: 0.011135
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0374 | R² TEST: -0.0156 ***
RMSE: 0.125909 | Features: 50
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_02_training.png
  └─ Overfitting status: Moderate (Gap: 20.8%)

=== Window 3 ===
Train: 1963-01 to 1982-12 (82418 samples)
Val:   1983-01 to 1987-12 (48981 samples)
Test:  1988-01 to 1988-12 (12406 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (82418, 234)
Data cleaned successfully
→ Using min_samples_split=824, min_samples_leaf=412 for 82418 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 65934 training samples, 16484 validation samples
  20 features: Time-based R² = -0.1123
  25 features: Time-based R² = -0.1126
  30 features: Time-based R² = -0.0704
  35 features: Time-based R² = -0.0520
  40 features: Time-based R² = -0.0529
  45 features: Time-based R² = -0.0739
  50 features: Time-based R² = -0.0679
Optimal number of features: 35 (Time-based R² = -0.0520)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012057
  *** New best score: 0.012057

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011940
  *** New best score: 0.011940

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012127

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012020

Grid search completed!
Best validation MSE: 0.011940
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1156 | R² TEST: -0.0123 ***
RMSE: 0.098919 | Features: 35
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_03_training.png
  └─ Overfitting status: Moderate (Gap: 33.5%)

=== Window 4 ===
Train: 1964-01 to 1983-12 (89067 samples)
Val:   1984-01 to 1988-12 (53019 samples)
Test:  1989-01 to 1989-12 (13048 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (89067, 234)
Data cleaned successfully
→ Using min_samples_split=890, min_samples_leaf=445 for 89067 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 71253 training samples, 17814 validation samples
  20 features: Time-based R² = -0.0675
  25 features: Time-based R² = -0.0436
  30 features: Time-based R² = 0.0061
  35 features: Time-based R² = -0.0401
  40 features: Time-based R² = -0.0201
  45 features: Time-based R² = -0.0243
  50 features: Time-based R² = -0.0274
Optimal number of features: 30 (Time-based R² = 0.0061)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011884
  *** New best score: 0.011884

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011820
  *** New best score: 0.011820

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011650
  *** New best score: 0.011650

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011804

Grid search completed!
Best validation MSE: 0.011650
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0394 | R² TEST: -0.0031 ***
RMSE: 0.098410 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_04_training.png
  └─ Overfitting status: Moderate (Gap: 23.3%)

=== Window 5 ===
Train: 1965-01 to 1984-12 (96296 samples)
Val:   1985-01 to 1989-12 (56989 samples)
Test:  1990-01 to 1990-12 (13617 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (96296, 234)
Data cleaned successfully
→ Using min_samples_split=962, min_samples_leaf=481 for 96296 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 77036 training samples, 19260 validation samples
  20 features: Time-based R² = -0.0252
  25 features: Time-based R² = -0.0047
  30 features: Time-based R² = -0.0027
  35 features: Time-based R² = 0.0193
  40 features: Time-based R² = 0.0304
  45 features: Time-based R² = 0.0213
  50 features: Time-based R² = 0.0171
Optimal number of features: 40 (Time-based R² = 0.0304)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011538
  *** New best score: 0.011538

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011565

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011509
  *** New best score: 0.011509

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011428
  *** New best score: 0.011428

Grid search completed!
Best validation MSE: 0.011428
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0237 | R² TEST: -0.0109 ***
RMSE: 0.118679 | Features: 40
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_05_training.png
  └─ Overfitting status: Minimal (Gap: 14.5%)

=== Window 6 ===
Train: 1966-01 to 1985-12 (103898 samples)
Val:   1986-01 to 1990-12 (60987 samples)
Test:  1991-01 to 1991-12 (14227 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (103898, 234)
Data cleaned successfully
→ Using min_samples_split=1038, min_samples_leaf=519 for 103898 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 83118 training samples, 20780 validation samples
  20 features: Time-based R² = -0.0456
  25 features: Time-based R² = -0.0776
  30 features: Time-based R² = -0.1023
  35 features: Time-based R² = -0.1266
  40 features: Time-based R² = -0.1002
  45 features: Time-based R² = -0.0690
  50 features: Time-based R² = -0.0547
Optimal number of features: 20 (Time-based R² = -0.0456)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012536
  *** New best score: 0.012536

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012260
  *** New best score: 0.012260

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012603

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012434

Grid search completed!
Best validation MSE: 0.012260
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0268 | R² TEST: -0.0305 ***
RMSE: 0.118215 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_06_training.png
  └─ Overfitting status: Moderate (Gap: 24.4%)

=== Window 7 ===
Train: 1967-01 to 1986-12 (111671 samples)
Val:   1987-01 to 1991-12 (64851 samples)
Test:  1992-01 to 1992-12 (15449 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (111671, 234)
Data cleaned successfully
→ Using min_samples_split=1116, min_samples_leaf=558 for 111671 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 89336 training samples, 22335 validation samples
  20 features: Time-based R² = 0.0106
  25 features: Time-based R² = -0.0496
  30 features: Time-based R² = -0.0368
  35 features: Time-based R² = -0.0446
  40 features: Time-based R² = -0.0523
  45 features: Time-based R² = -0.0588
  50 features: Time-based R² = -0.1924
Optimal number of features: 20 (Time-based R² = 0.0106)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012980
  *** New best score: 0.012980

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012979
  *** New best score: 0.012979

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012983

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012956
  *** New best score: 0.012956

Grid search completed!
Best validation MSE: 0.012956
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0011 | R² TEST: -0.0032 ***
RMSE: 0.106713 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_07_training.png
  └─ Overfitting status: Minimal (Gap: 18.5%)

=== Window 8 ===
Train: 1968-01 to 1987-12 (120310 samples)
Val:   1988-01 to 1992-12 (68747 samples)
Test:  1993-01 to 1993-12 (16943 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (120310, 234)
Data cleaned successfully
→ Using min_samples_split=1203, min_samples_leaf=601 for 120310 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 96248 training samples, 24062 validation samples
  20 features: Time-based R² = -0.0877
  25 features: Time-based R² = -0.0241
  30 features: Time-based R² = -0.0134
  35 features: Time-based R² = -0.0007
  40 features: Time-based R² = -0.0271
  45 features: Time-based R² = -0.0412
  50 features: Time-based R² = -0.0400
Optimal number of features: 35 (Time-based R² = -0.0007)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012334
  *** New best score: 0.012334

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012324
  *** New best score: 0.012324

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012436

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012524

Grid search completed!
Best validation MSE: 0.012324
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0372 | R² TEST: -0.0257 ***
RMSE: 0.107725 | Features: 35
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_08_training.png
  └─ Overfitting status: Minimal (Gap: 10.4%)

=== Window 9 ===
Train: 1969-01 to 1988-12 (129677 samples)
Val:   1989-01 to 1993-12 (73284 samples)
Test:  1994-01 to 1994-12 (18730 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (129677, 234)
Data cleaned successfully
→ Using min_samples_split=1296, min_samples_leaf=648 for 129677 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 103741 training samples, 25936 validation samples
  20 features: Time-based R² = -0.0428
  25 features: Time-based R² = -0.0412
  30 features: Time-based R² = -0.0861
  35 features: Time-based R² = -0.0906
  40 features: Time-based R² = -0.0389
  45 features: Time-based R² = -0.0445
  50 features: Time-based R² = -0.0802
Optimal number of features: 40 (Time-based R² = -0.0389)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012822
  *** New best score: 0.012822

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012687
  *** New best score: 0.012687

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012646
  *** New best score: 0.012646

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012603
  *** New best score: 0.012603

Grid search completed!
Best validation MSE: 0.012603
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0681 | R² TEST: -0.0268 ***
RMSE: 0.100864 | Features: 40
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_09_training.png
  └─ Overfitting status: Minimal (Gap: 15.6%)

=== Window 10 ===
Train: 1970-01 to 1989-12 (139494 samples)
Val:   1990-01 to 1994-12 (78966 samples)
Test:  1995-01 to 1995-12 (19857 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (139494, 234)
Data cleaned successfully
→ Using min_samples_split=1394, min_samples_leaf=697 for 139494 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 111595 training samples, 27899 validation samples
  20 features: Time-based R² = -0.0878
  25 features: Time-based R² = -0.0992
  30 features: Time-based R² = -0.1009
  35 features: Time-based R² = -0.0955
  40 features: Time-based R² = -0.0835
  45 features: Time-based R² = -0.0721
  50 features: Time-based R² = -0.1062
Optimal number of features: 45 (Time-based R² = -0.0721)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012497
  *** New best score: 0.012497

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012502

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012383
  *** New best score: 0.012383

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012404

Grid search completed!
Best validation MSE: 0.012383
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1028 | R² TEST: -0.0124 ***
RMSE: 0.105954 | Features: 45
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_10_training.png
  └─ Overfitting status: Minimal (Gap: 17.0%)

=== Window 11 ===
Train: 1971-01 to 1990-12 (149676 samples)
Val:   1991-01 to 1995-12 (85206 samples)
Test:  1996-01 to 1996-12 (21557 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (149676, 234)
Data cleaned successfully
→ Using min_samples_split=1496, min_samples_leaf=748 for 149676 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 119740 training samples, 29936 validation samples
  20 features: Time-based R² = 0.0711
  25 features: Time-based R² = 0.0372
  30 features: Time-based R² = 0.0481
  35 features: Time-based R² = 0.0340
  40 features: Time-based R² = 0.0536
  45 features: Time-based R² = 0.0445
  50 features: Time-based R² = 0.0407
Optimal number of features: 20 (Time-based R² = 0.0711)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011917
  *** New best score: 0.011917

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011980

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012000

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011973

Grid search completed!
Best validation MSE: 0.011917
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0555 | R² TEST: -0.0152 ***
RMSE: 0.111958 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_11_training.png
  └─ Overfitting status: Minimal (Gap: 5.5%)

=== Window 12 ===
Train: 1972-01 to 1991-12 (160253 samples)
Val:   1992-01 to 1996-12 (92536 samples)
Test:  1997-01 to 1997-12 (23201 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (160253, 234)
Data cleaned successfully
→ Using min_samples_split=1602, min_samples_leaf=801 for 160253 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 128202 training samples, 32051 validation samples
  20 features: Time-based R² = 0.0814
  25 features: Time-based R² = 0.0790
  30 features: Time-based R² = 0.0821
  35 features: Time-based R² = 0.0791
  40 features: Time-based R² = 0.0718
  45 features: Time-based R² = 0.0740
  50 features: Time-based R² = 0.0760
Optimal number of features: 30 (Time-based R² = 0.0821)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011697
  *** New best score: 0.011697

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011921

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011672
  *** New best score: 0.011672

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011712

Grid search completed!
Best validation MSE: 0.011672
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0755 | R² TEST: -0.0020 ***
RMSE: 0.117407 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_12_training.png
  └─ Overfitting status: Minimal (Gap: 0.3%)

=== Window 13 ===
Train: 1973-01 to 1992-12 (171807 samples)
Val:   1993-01 to 1997-12 (100288 samples)
Test:  1998-01 to 1998-12 (24897 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (171807, 234)
Data cleaned successfully
→ Using min_samples_split=1718, min_samples_leaf=859 for 171807 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 137445 training samples, 34362 validation samples
  20 features: Time-based R² = -0.0389
  25 features: Time-based R² = 0.0055
  30 features: Time-based R² = -0.0090
  35 features: Time-based R² = 0.0035
  40 features: Time-based R² = -0.0054
  45 features: Time-based R² = -0.0054
  50 features: Time-based R² = -0.0083
Optimal number of features: 25 (Time-based R² = 0.0055)
Selected 25 optimal features from Random Forest
Feature reduction: 234 → 25 (10.7%)
Selected 25 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012518
  *** New best score: 0.012518

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012135
  *** New best score: 0.012135

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012270

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012250

Grid search completed!
Best validation MSE: 0.012135
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0376 | R² TEST: -0.0082 ***
RMSE: 0.141827 | Features: 25
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_13_training.png
  └─ Overfitting status: Minimal (Gap: -0.4%)

=== Window 14 ===
Train: 1974-01 to 1993-12 (184034 samples)
Val:   1994-01 to 1998-12 (108242 samples)
Test:  1999-01 to 1999-12 (25977 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (184034, 234)
Data cleaned successfully
→ Using min_samples_split=1840, min_samples_leaf=920 for 184034 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 147227 training samples, 36807 validation samples
  20 features: Time-based R² = -0.0415
  25 features: Time-based R² = -0.0311
  30 features: Time-based R² = -0.0183
  35 features: Time-based R² = -0.0083
  40 features: Time-based R² = -0.0144
  45 features: Time-based R² = -0.0157
  50 features: Time-based R² = -0.0096
Optimal number of features: 35 (Time-based R² = -0.0083)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.014014
  *** New best score: 0.014014

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.014120

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.014043

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.014122

Grid search completed!
Best validation MSE: 0.014014
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0255 | R² TEST: -0.0066 ***
RMSE: 0.139978 | Features: 35
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_14_training.png
  └─ Overfitting status: Minimal (Gap: 15.4%)

=== Window 15 ===
Train: 1975-01 to 1994-12 (198005 samples)
Val:   1995-01 to 1999-12 (115489 samples)
Test:  2000-01 to 2000-12 (27607 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (198005, 234)
Data cleaned successfully
→ Using min_samples_split=1980, min_samples_leaf=990 for 198005 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 158404 training samples, 39601 validation samples
  20 features: Time-based R² = -0.0573
  25 features: Time-based R² = -0.0549
  30 features: Time-based R² = -0.0685
  35 features: Time-based R² = -0.0524
  40 features: Time-based R² = -0.0282
  45 features: Time-based R² = -0.0573
  50 features: Time-based R² = -0.0573
Optimal number of features: 40 (Time-based R² = -0.0282)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015598
  *** New best score: 0.015598

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015644

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015627

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015695

Grid search completed!
Best validation MSE: 0.015598
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0541 | R² TEST: -0.0075 ***
RMSE: 0.156318 | Features: 40
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_15_training.png
  └─ Overfitting status: Moderate (Gap: 37.4%)

=== Window 16 ===
Train: 1976-01 to 1995-12 (213078 samples)
Val:   1996-01 to 2000-12 (123239 samples)
Test:  2001-01 to 2001-12 (28903 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (213078, 234)
Data cleaned successfully
→ Using min_samples_split=2130, min_samples_leaf=1065 for 213078 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 170462 training samples, 42616 validation samples
  20 features: Time-based R² = -0.0357
  25 features: Time-based R² = -0.0341
  30 features: Time-based R² = -0.0282
  35 features: Time-based R² = -0.0403
  40 features: Time-based R² = -0.0664
  45 features: Time-based R² = -0.0547
  50 features: Time-based R² = -0.0641
Optimal number of features: 30 (Time-based R² = -0.0282)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.018304
  *** New best score: 0.018304

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.018296
  *** New best score: 0.018296

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.018310

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.018525

Grid search completed!
Best validation MSE: 0.018296
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0491 | R² TEST: -0.0095 ***
RMSE: 0.139813 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_16_training.png
  └─ Overfitting status: Severe (Gap: 59.5%)

=== Window 17 ===
Train: 1977-01 to 1996-12 (229767 samples)
Val:   1997-01 to 2001-12 (130585 samples)
Test:  2002-01 to 2002-12 (30051 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (229767, 234)
Data cleaned successfully
→ Using min_samples_split=2297, min_samples_leaf=1148 for 229767 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 183813 training samples, 45954 validation samples
  20 features: Time-based R² = -0.0370
  25 features: Time-based R² = -0.0318
  30 features: Time-based R² = -0.0257
  35 features: Time-based R² = -0.0089
  40 features: Time-based R² = -0.0050
  45 features: Time-based R² = 0.0016
  50 features: Time-based R² = -0.0047
Optimal number of features: 45 (Time-based R² = 0.0016)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.019819
  *** New best score: 0.019819

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.019900

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.019933

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.019552
  *** New best score: 0.019552

Grid search completed!
Best validation MSE: 0.019552
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0185 | R² TEST: -0.0110 ***
RMSE: 0.131525 | Features: 45
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_17_training.png
  └─ Overfitting status: Severe (Gap: 63.8%)

=== Window 18 ===
Train: 1978-01 to 1997-12 (247961 samples)
Val:   1998-01 to 2002-12 (137435 samples)
Test:  2003-01 to 2003-12 (30738 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (247961, 234)
Data cleaned successfully
→ Using min_samples_split=2479, min_samples_leaf=1239 for 247961 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 198368 training samples, 49593 validation samples
  20 features: Time-based R² = -0.0443
  25 features: Time-based R² = -0.0441
  30 features: Time-based R² = -0.0412
  35 features: Time-based R² = -0.0649
  40 features: Time-based R² = -0.0628
  45 features: Time-based R² = -0.0810
  50 features: Time-based R² = -0.0579
Optimal number of features: 30 (Time-based R² = -0.0412)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.020308
  *** New best score: 0.020308

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.020367

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.020366

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.020846

Grid search completed!
Best validation MSE: 0.020308
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0748 | R² TEST: -0.0994 ***
RMSE: 0.121023 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_18_training.png
  └─ Overfitting status: Severe (Gap: 73.0%)

=== Window 19 ===
Train: 1979-01 to 1998-12 (267762 samples)
Val:   1999-01 to 2003-12 (143276 samples)
Test:  2004-01 to 2004-12 (31983 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (267762, 234)
Data cleaned successfully
→ Using min_samples_split=2677, min_samples_leaf=1338 for 267762 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 214209 training samples, 53553 validation samples
  20 features: Time-based R² = 0.0243
  25 features: Time-based R² = 0.0295
  30 features: Time-based R² = 0.0323
  35 features: Time-based R² = 0.0266
  40 features: Time-based R² = 0.0200
  45 features: Time-based R² = 0.0200
  50 features: Time-based R² = 0.0226
Optimal number of features: 30 (Time-based R² = 0.0323)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.019907
  *** New best score: 0.019907

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.020136

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.019672
  *** New best score: 0.019672

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.019809

Grid search completed!
Best validation MSE: 0.019672
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0383 | R² TEST: -0.0591 ***
RMSE: 0.104151 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_19_training.png
  └─ Overfitting status: Severe (Gap: 50.6%)

=== Window 20 ===
Train: 1980-01 to 1999-12 (288545 samples)
Val:   2000-01 to 2004-12 (149282 samples)
Test:  2005-01 to 2005-12 (33728 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (288545, 234)
Data cleaned successfully
→ Using min_samples_split=2885, min_samples_leaf=1442 for 288545 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 230836 training samples, 57709 validation samples
  20 features: Time-based R² = 0.0302
  25 features: Time-based R² = 0.0264
  30 features: Time-based R² = -0.0795
  35 features: Time-based R² = -0.0341
  40 features: Time-based R² = -0.0358
  45 features: Time-based R² = -0.0317
  50 features: Time-based R² = -0.0341
Optimal number of features: 20 (Time-based R² = 0.0302)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.018268
  *** New best score: 0.018268

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.018156
  *** New best score: 0.018156

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.018009
  *** New best score: 0.018009

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.018322

Grid search completed!
Best validation MSE: 0.018009
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0228 | R² TEST: -0.0025 ***
RMSE: 0.100303 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_20_training.png
  └─ Overfitting status: Moderate (Gap: 25.5%)

=== Window 21 ===
Train: 1981-01 to 2000-12 (310887 samples)
Val:   2001-01 to 2005-12 (155403 samples)
Test:  2006-01 to 2006-12 (35831 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (310887, 234)
Data cleaned successfully
→ Using min_samples_split=3108, min_samples_leaf=1554 for 310887 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 248709 training samples, 62178 validation samples
  20 features: Time-based R² = 0.0008
  25 features: Time-based R² = -0.0023
  30 features: Time-based R² = 0.0020
  35 features: Time-based R² = 0.0014
  40 features: Time-based R² = 0.0003
  45 features: Time-based R² = 0.0066
  50 features: Time-based R² = 0.0065
Optimal number of features: 45 (Time-based R² = 0.0066)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015451
  *** New best score: 0.015451

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015289
  *** New best score: 0.015289

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015391

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015355

Grid search completed!
Best validation MSE: 0.015289
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0417 | R² TEST: -0.0129 ***
RMSE: 0.092057 | Features: 45
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_21_training.png
  └─ Overfitting status: Minimal (Gap: -0.6%)

=== Window 22 ===
Train: 1982-01 to 2001-12 (333267 samples)
Val:   2002-01 to 2006-12 (162331 samples)
Test:  2007-01 to 2007-12 (39105 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (333267, 234)
Data cleaned successfully
→ Using min_samples_split=3332, min_samples_leaf=1666 for 333267 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 266613 training samples, 66654 validation samples
  20 features: Time-based R² = -0.0190
  25 features: Time-based R² = -0.0356
  30 features: Time-based R² = -0.0117
  35 features: Time-based R² = -0.0265
  40 features: Time-based R² = -0.0239
  45 features: Time-based R² = -0.0296
  50 features: Time-based R² = -0.0205
Optimal number of features: 30 (Time-based R² = -0.0117)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012837
  *** New best score: 0.012837

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012813
  *** New best score: 0.012813

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012765
  *** New best score: 0.012765

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012728
  *** New best score: 0.012728

Grid search completed!
Best validation MSE: 0.012728
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0224 | R² TEST: -0.0374 ***
RMSE: 0.102455 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_22_training.png
  └─ Overfitting status: Minimal (Gap: -23.9%)

=== Window 23 ===
Train: 1983-01 to 2002-12 (355451 samples)
Val:   2003-01 to 2007-12 (171385 samples)
Test:  2008-01 to 2008-12 (40868 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (355451, 234)
Data cleaned successfully
→ Using min_samples_split=3554, min_samples_leaf=1777 for 355451 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 284360 training samples, 71091 validation samples
  20 features: Time-based R² = -0.1145
  25 features: Time-based R² = -0.1332
  30 features: Time-based R² = -0.1332
  35 features: Time-based R² = -0.1394
  40 features: Time-based R² = -0.1575
  45 features: Time-based R² = -0.1376
  50 features: Time-based R² = -0.1423
Optimal number of features: 20 (Time-based R² = -0.1145)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011075
  *** New best score: 0.011075

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010978
  *** New best score: 0.010978

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011119

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011119

Grid search completed!
Best validation MSE: 0.010978
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0563 | R² TEST: -0.0779 ***
RMSE: 0.154516 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_23_training.png
  └─ Overfitting status: Minimal (Gap: -33.5%)

=== Window 24 ===
Train: 1984-01 to 2003-12 (377821 samples)
Val:   2004-01 to 2008-12 (181515 samples)
Test:  2009-01 to 2009-12 (41319 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (377821, 234)
Data cleaned successfully
→ Using min_samples_split=3778, min_samples_leaf=1889 for 377821 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 302256 training samples, 75565 validation samples
  20 features: Time-based R² = -0.0402
  25 features: Time-based R² = -0.0286
  30 features: Time-based R² = -0.0507
  35 features: Time-based R² = -0.0283
  40 features: Time-based R² = -0.0405
  45 features: Time-based R² = -0.0671
  50 features: Time-based R² = -0.0496
Optimal number of features: 35 (Time-based R² = -0.0283)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013283
  *** New best score: 0.013283

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.013025
  *** New best score: 0.013025

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013372

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.013318

Grid search completed!
Best validation MSE: 0.013025
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0497 | R² TEST: -0.0236 ***
RMSE: 0.149647 | Features: 35
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_24_training.png
  └─ Overfitting status: Minimal (Gap: -20.9%)

=== Window 25 ===
Train: 1985-01 to 2004-12 (400726 samples)
Val:   2005-01 to 2009-12 (190851 samples)
Test:  2010-01 to 2010-12 (43249 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (400726, 234)
Data cleaned successfully
→ Using min_samples_split=4007, min_samples_leaf=2003 for 400726 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 320580 training samples, 80146 validation samples
  20 features: Time-based R² = -0.0372
  25 features: Time-based R² = -0.0267
  30 features: Time-based R² = -0.0105
  35 features: Time-based R² = -0.0177
  40 features: Time-based R² = -0.0240
  45 features: Time-based R² = -0.0301
  50 features: Time-based R² = -0.0441
Optimal number of features: 30 (Time-based R² = -0.0105)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015790
  *** New best score: 0.015790

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015509
  *** New best score: 0.015509

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015672

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015942

Grid search completed!
Best validation MSE: 0.015509
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0325 | R² TEST: -0.0208 ***
RMSE: 0.112454 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_25_training.png
  └─ Overfitting status: Minimal (Gap: -5.3%)

=== Window 26 ===
Train: 1986-01 to 2005-12 (424835 samples)
Val:   2006-01 to 2010-12 (200372 samples)
Test:  2011-01 to 2011-12 (45523 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (424835, 234)
Data cleaned successfully
→ Using min_samples_split=4248, min_samples_leaf=2124 for 424835 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 339868 training samples, 84967 validation samples
  20 features: Time-based R² = -0.1108
  25 features: Time-based R² = -0.1507
  30 features: Time-based R² = -0.1404
  35 features: Time-based R² = -0.0999
  40 features: Time-based R² = -0.0202
  45 features: Time-based R² = -0.0305
  50 features: Time-based R² = -0.0304
Optimal number of features: 40 (Time-based R² = -0.0202)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015641
  *** New best score: 0.015641

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015814

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015942

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015965

Grid search completed!
Best validation MSE: 0.015641
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0623 | R² TEST: -0.0711 ***
RMSE: 0.115318 | Features: 40
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_26_training.png
  └─ Overfitting status: Minimal (Gap: 0.3%)

=== Window 27 ===
Train: 1987-01 to 2006-12 (450303 samples)
Val:   2007-01 to 2011-12 (210064 samples)
Test:  2012-01 to 2012-12 (47466 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (450303, 234)
Data cleaned successfully
→ Using min_samples_split=4503, min_samples_leaf=2251 for 450303 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 360242 training samples, 90061 validation samples
  20 features: Time-based R² = -0.1028
  25 features: Time-based R² = -0.0443
  30 features: Time-based R² = -0.0384
  35 features: Time-based R² = -0.0331
  40 features: Time-based R² = -0.0311
  45 features: Time-based R² = -0.0161
  50 features: Time-based R² = -0.0248
Optimal number of features: 45 (Time-based R² = -0.0161)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.016154
  *** New best score: 0.016154

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.016145
  *** New best score: 0.016145

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.016470

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.016302

Grid search completed!
Best validation MSE: 0.016145
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1118 | R² TEST: -0.0444 ***
RMSE: 0.098364 | Features: 45
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_27_training.png
  └─ Overfitting status: Minimal (Gap: 13.4%)

=== Window 28 ===
Train: 1988-01 to 2007-12 (477855 samples)
Val:   2008-01 to 2012-12 (218425 samples)
Test:  2013-01 to 2013-12 (49097 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (477855, 234)
Data cleaned successfully
→ Using min_samples_split=4778, min_samples_leaf=2389 for 477855 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 382284 training samples, 95571 validation samples
  20 features: Time-based R² = -0.0436
  25 features: Time-based R² = -0.0635
  30 features: Time-based R² = -0.0361
  35 features: Time-based R² = -0.0398
  40 features: Time-based R² = -0.0345
  45 features: Time-based R² = -0.0256
  50 features: Time-based R² = -0.0415
Optimal number of features: 45 (Time-based R² = -0.0256)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015765
  *** New best score: 0.015765

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015553
  *** New best score: 0.015553

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015941

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015793

Grid search completed!
Best validation MSE: 0.015553
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0380 | R² TEST: -0.0182 ***
RMSE: 0.094242 | Features: 45
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_28_training.png
  └─ Overfitting status: Minimal (Gap: 8.3%)

=== Window 29 ===
Train: 1989-01 to 2008-12 (506317 samples)
Val:   2009-01 to 2013-12 (226654 samples)
Test:  2014-01 to 2014-12 (52323 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (506317, 234)
Data cleaned successfully
→ Using min_samples_split=5063, min_samples_leaf=2531 for 506317 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 405053 training samples, 101264 validation samples
  20 features: Time-based R² = -0.0425
  25 features: Time-based R² = -0.0261
  30 features: Time-based R² = -0.0100
  35 features: Time-based R² = -0.0061
  40 features: Time-based R² = -0.0244
  45 features: Time-based R² = -0.1304
  50 features: Time-based R² = -0.1313
Optimal number of features: 35 (Time-based R² = -0.0061)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013107
  *** New best score: 0.013107

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.012878
  *** New best score: 0.012878

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.012824
  *** New best score: 0.012824

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.013002

Grid search completed!
Best validation MSE: 0.012824
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0704 | R² TEST: -0.0176 ***
RMSE: 0.093945 | Features: 35
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_29_training.png
  └─ Overfitting status: Minimal (Gap: -13.3%)

=== Window 30 ===
Train: 1990-01 to 2009-12 (534588 samples)
Val:   2010-01 to 2014-12 (237658 samples)
Test:  2015-01 to 2015-12 (55529 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (534588, 234)
Data cleaned successfully
→ Using min_samples_split=5345, min_samples_leaf=2672 for 534588 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 427670 training samples, 106918 validation samples
  20 features: Time-based R² = -0.0194
  25 features: Time-based R² = -0.0239
  30 features: Time-based R² = -0.0089
  35 features: Time-based R² = -0.0203
  40 features: Time-based R² = -0.0077
  45 features: Time-based R² = -0.0255
  50 features: Time-based R² = -0.0116
Optimal number of features: 40 (Time-based R² = -0.0077)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010591
  *** New best score: 0.010591

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010522
  *** New best score: 0.010522

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010822

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010939

Grid search completed!
Best validation MSE: 0.010522
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0831 | R² TEST: -0.0697 ***
RMSE: 0.109438 | Features: 40
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_30_training.png
  └─ Overfitting status: Minimal (Gap: -32.1%)

=== Window 31 ===
Train: 1991-01 to 2010-12 (564220 samples)
Val:   2011-01 to 2015-12 (249938 samples)
Test:  2016-01 to 2016-12 (58133 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (564220, 234)
Data cleaned successfully
→ Using min_samples_split=5642, min_samples_leaf=2821 for 564220 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 451376 training samples, 112844 validation samples
  20 features: Time-based R² = -0.0539
  25 features: Time-based R² = -0.0332
  30 features: Time-based R² = -0.2124
  35 features: Time-based R² = -0.1962
  40 features: Time-based R² = -0.1702
  45 features: Time-based R² = -0.1346
  50 features: Time-based R² = -0.1439
Optimal number of features: 25 (Time-based R² = -0.0332)
Selected 25 optimal features from Random Forest
Feature reduction: 234 → 25 (10.7%)
Selected 25 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010263
  *** New best score: 0.010263

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010442

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011123

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010848

Grid search completed!
Best validation MSE: 0.010263
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0764 | R² TEST: -0.0649 ***
RMSE: 0.110453 | Features: 25
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_31_training.png
  └─ Overfitting status: Minimal (Gap: -30.0%)

=== Window 32 ===
Train: 1992-01 to 2011-12 (595516 samples)
Val:   2012-01 to 2016-12 (262548 samples)
Test:  2017-01 to 2017-12 (61214 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (595516, 234)
Data cleaned successfully
→ Using min_samples_split=5955, min_samples_leaf=2977 for 595516 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 476412 training samples, 119104 validation samples
  20 features: Time-based R² = -0.0107
  25 features: Time-based R² = -0.0047
  30 features: Time-based R² = -0.4285
  35 features: Time-based R² = -0.1084
  40 features: Time-based R² = -0.1448
  45 features: Time-based R² = -0.1142
  50 features: Time-based R² = -0.0996
Optimal number of features: 25 (Time-based R² = -0.0047)
Selected 25 optimal features from Random Forest
Feature reduction: 234 → 25 (10.7%)
Selected 25 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010396
  *** New best score: 0.010396

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010246
  *** New best score: 0.010246

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010432

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010137
  *** New best score: 0.010137

Grid search completed!
Best validation MSE: 0.010137
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0653 | R² TEST: -0.0655 ***
RMSE: 0.096566 | Features: 25
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_32_training.png
  └─ Overfitting status: Minimal (Gap: -29.5%)

=== Window 33 ===
Train: 1993-01 to 2012-12 (627533 samples)
Val:   2013-01 to 2017-12 (276296 samples)
Test:  2018-01 to 2018-12 (65313 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (627533, 234)
Data cleaned successfully
→ Using min_samples_split=6275, min_samples_leaf=3137 for 627533 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 502026 training samples, 125507 validation samples
  20 features: Time-based R² = -0.0124
  25 features: Time-based R² = -0.0105
  30 features: Time-based R² = -0.0269
  35 features: Time-based R² = -0.0274
  40 features: Time-based R² = -0.0260
  45 features: Time-based R² = -0.0212
  50 features: Time-based R² = -0.0009
Optimal number of features: 50 (Time-based R² = -0.0009)
Selected 50 optimal features from Random Forest
Feature reduction: 234 → 50 (21.4%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010255
  *** New best score: 0.010255

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010440

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010047
  *** New best score: 0.010047

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010123

Grid search completed!
Best validation MSE: 0.010047
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0810 | R² TEST: -0.0289 ***
RMSE: 0.112889 | Features: 50
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_33_training.png
  └─ Overfitting status: Minimal (Gap: -29.4%)

=== Window 34 ===
Train: 1994-01 to 2013-12 (659687 samples)
Val:   2014-01 to 2018-12 (292512 samples)
Test:  2019-01 to 2019-12 (69500 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (659687, 234)
Data cleaned successfully
→ Using min_samples_split=6596, min_samples_leaf=3298 for 659687 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 527749 training samples, 131938 validation samples
  20 features: Time-based R² = 0.0203
  25 features: Time-based R² = 0.0130
  30 features: Time-based R² = -0.0351
  35 features: Time-based R² = -0.0761
  40 features: Time-based R² = -0.0692
  45 features: Time-based R² = -0.1069
  50 features: Time-based R² = -0.0674
Optimal number of features: 20 (Time-based R² = 0.0203)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010628
  *** New best score: 0.010628

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010766

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.010899

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.010952

Grid search completed!
Best validation MSE: 0.010628
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0676 | R² TEST: -0.0535 ***
RMSE: 0.110732 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_34_training.png
  └─ Overfitting status: Minimal (Gap: -21.7%)

=== Window 35 ===
Train: 1995-01 to 2014-12 (693280 samples)
Val:   2015-01 to 2019-12 (309689 samples)
Test:  2020-01 to 2020-12 (73804 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (693280, 234)
Data cleaned successfully
→ Using min_samples_split=6932, min_samples_leaf=3466 for 693280 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 554624 training samples, 138656 validation samples
  20 features: Time-based R² = -0.0772
  25 features: Time-based R² = -0.0358
  30 features: Time-based R² = -0.0272
  35 features: Time-based R² = -0.0572
  40 features: Time-based R² = -0.0560
  45 features: Time-based R² = -0.0529
  50 features: Time-based R² = -0.0315
Optimal number of features: 30 (Time-based R² = -0.0272)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011454
  *** New best score: 0.011454

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011229
  *** New best score: 0.011229

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.011487

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.011452

Grid search completed!
Best validation MSE: 0.011229
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0971 | R² TEST: -0.0012 ***
RMSE: 0.151568 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_35_training.png
  └─ Overfitting status: Minimal (Gap: -14.0%)

=== Window 36 ===
Train: 1996-01 to 2015-12 (728952 samples)
Val:   2016-01 to 2020-12 (327964 samples)
Test:  2021-01 to 2021-12 (83029 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (728952, 234)
Data cleaned successfully
→ Using min_samples_split=7289, min_samples_leaf=3644 for 728952 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 583161 training samples, 145791 validation samples
  20 features: Time-based R² = -0.0079
  25 features: Time-based R² = -0.0016
  30 features: Time-based R² = 0.0019
  35 features: Time-based R² = -0.0113
  40 features: Time-based R² = -0.0119
  45 features: Time-based R² = -0.0319
  50 features: Time-based R² = -0.0199
Optimal number of features: 30 (Time-based R² = 0.0019)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013901
  *** New best score: 0.013901

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.013683
  *** New best score: 0.013683

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013854

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.013806

Grid search completed!
Best validation MSE: 0.013683
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1161 | R² TEST: -0.0553 ***
RMSE: 0.116899 | Features: 30
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_36_training.png
  └─ Overfitting status: Minimal (Gap: 7.1%)

=== Window 37 ===
Train: 1997-01 to 2016-12 (765528 samples)
Val:   2017-01 to 2021-12 (352860 samples)
Test:  2022-01 to 2022-12 (92602 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (765528, 234)
Data cleaned successfully
→ Using min_samples_split=7655, min_samples_leaf=3827 for 765528 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 612422 training samples, 153106 validation samples
  20 features: Time-based R² = -0.0370
  25 features: Time-based R² = -0.0378
  30 features: Time-based R² = -0.0954
  35 features: Time-based R² = -0.0632
  40 features: Time-based R² = -0.0598
  45 features: Time-based R² = -0.0481
  50 features: Time-based R² = -0.0426
Optimal number of features: 20 (Time-based R² = -0.0370)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013978
  *** New best score: 0.013978

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.013931
  *** New best score: 0.013931

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.013982

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.014116

Grid search completed!
Best validation MSE: 0.013931
Best parameters: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0657 | R² TEST: -0.0401 ***
RMSE: 0.141190 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_37_training.png
  └─ Overfitting status: Minimal (Gap: 5.6%)

=== Window 38 ===
Train: 1998-01 to 2017-12 (803541 samples)
Val:   2018-01 to 2022-12 (384248 samples)
Test:  2023-01 to 2023-12 (99011 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (803541, 234)
Data cleaned successfully
→ Using min_samples_split=8035, min_samples_leaf=4017 for 803541 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 642832 training samples, 160709 validation samples
  20 features: Time-based R² = -0.0035
  25 features: Time-based R² = -0.0164
  30 features: Time-based R² = -0.0175
  35 features: Time-based R² = -0.0714
  40 features: Time-based R² = -0.0696
  45 features: Time-based R² = -0.0616
  50 features: Time-based R² = -0.0951
Optimal number of features: 20 (Time-based R² = -0.0035)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 4 parameter combinations:
  l1_lambda: [1e-06, 1e-05]
  weight_decay: [1e-06, 1e-05]

Combination 1/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.015869
  *** New best score: 0.015869

Combination 2/4: {'l1_lambda': 1e-06, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.016177

Combination 3/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-06}
Training ensemble model 1/1
  Validation MSE: 0.016257

Combination 4/4: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Training ensemble model 1/1
  Validation MSE: 0.015864
  *** New best score: 0.015864

Grid search completed!
Best validation MSE: 0.015864
Best parameters: {'l1_lambda': 1e-05, 'weight_decay': 1e-05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0606 | R² TEST: -0.0315 ***
RMSE: 0.131148 | Features: 20
  └─ Training plot saved: /home/truttman/RF_NN3/results/plots/individual_windows/window_38_training.png
  └─ Overfitting status: Moderate (Gap: 26.2%)

============================================================
FINAL RESULTS - PYTORCH ENSEMBLE
============================================================
Overall Test R²:     -0.0183
Overall Train R²:    0.0554
Overall RMSE:        0.1203
Average Window Test R²:  -0.0300
Average Window Train R²: 0.0554
Number of windows:   38
Average epochs:      34.5
Average features:    32.2
Feature reduction:   86.2%

R² BY WINDOW:
  Window  1: Train R² = 0.0043 | Test R² = -0.0442
  Window  2: Train R² = 0.0374 | Test R² = -0.0156
  Window  3: Train R² = 0.1156 | Test R² = -0.0123
  Window  4: Train R² = 0.0394 | Test R² = -0.0031
  Window  5: Train R² = 0.0237 | Test R² = -0.0109
  Window  6: Train R² = 0.0268 | Test R² = -0.0305
  Window  7: Train R² = 0.0011 | Test R² = -0.0032
  Window  8: Train R² = 0.0372 | Test R² = -0.0257
  Window  9: Train R² = 0.0681 | Test R² = -0.0268
  Window 10: Train R² = 0.1028 | Test R² = -0.0124
  Window 11: Train R² = 0.0555 | Test R² = -0.0152
  Window 12: Train R² = 0.0755 | Test R² = -0.0020
  Window 13: Train R² = 0.0376 | Test R² = -0.0082
  Window 14: Train R² = 0.0255 | Test R² = -0.0066
  Window 15: Train R² = 0.0541 | Test R² = -0.0075
  Window 16: Train R² = 0.0491 | Test R² = -0.0095
  Window 17: Train R² = 0.0185 | Test R² = -0.0110
  Window 18: Train R² = 0.0748 | Test R² = -0.0994
  Window 19: Train R² = 0.0383 | Test R² = -0.0591
  Window 20: Train R² = 0.0228 | Test R² = -0.0025
  Window 21: Train R² = 0.0417 | Test R² = -0.0129
  Window 22: Train R² = 0.0224 | Test R² = -0.0374
  Window 23: Train R² = 0.0563 | Test R² = -0.0779
  Window 24: Train R² = 0.0497 | Test R² = -0.0236
  Window 25: Train R² = 0.0325 | Test R² = -0.0208
  Window 26: Train R² = 0.0623 | Test R² = -0.0711
  Window 27: Train R² = 0.1118 | Test R² = -0.0444
  Window 28: Train R² = 0.0380 | Test R² = -0.0182
  Window 29: Train R² = 0.0704 | Test R² = -0.0176
  Window 30: Train R² = 0.0831 | Test R² = -0.0697
  Window 31: Train R² = 0.0764 | Test R² = -0.0649
  Window 32: Train R² = 0.0653 | Test R² = -0.0655
  Window 33: Train R² = 0.0810 | Test R² = -0.0289
  Window 34: Train R² = 0.0676 | Test R² = -0.0535
  Window 35: Train R² = 0.0971 | Test R² = -0.0012
  Window 36: Train R² = 0.1161 | Test R² = -0.0553
  Window 37: Train R² = 0.0657 | Test R² = -0.0401
  Window 38: Train R² = 0.0606 | Test R² = -0.0315
============================================================

5. SAVING RESULTS...
✓ Including permno data in predictions file
✓ Predictions saved: /home/truttman/RF_NN3/results/predictions.csv
✓ Window metrics saved: /home/truttman/RF_NN3/results/window_metrics.csv
✓ Feature evolution saved: /home/truttman/RF_NN3/results/feature_evolution.csv
✓ Overall results saved: /home/truttman/RF_NN3/results/overall_results.txt

6. GENERATING FINAL PLOTS...

=== GENERATING SUMMARY PLOTS ===
Main summary saved to: /home/truttman/RF_NN3/results/plots/main_summary.png
R² bar charts saved to: /home/truttman/RF_NN3/results/plots/r2_bar_charts.png
Performance analysis saved to: /home/truttman/RF_NN3/results/plots/performance_analysis.png
Predictions vs actual plot saved to: /home/truttman/RF_NN3/results/plots/predictions_vs_actual.png
✓ All plots saved to: /home/truttman/RF_NN3/results/plots

============================================================
FINAL SUMMARY
============================================================
Total windows processed: 38
Overall Test R²: -0.0183
Overall Train R²: 0.0554
Overall RMSE: 0.120268
Average features per window: 32.2
Average epochs per window: 34.5

Best Test R² Window: -0.0012
Worst Test R² Window: -0.0994
R² Standard Deviation: 0.0250

Results directory: /home/truttman/RF_NN3/results
Plots directory: /home/truttman/RF_NN3/results/plots

================================================================================
ANALYSIS COMPLETE!
================================================================================
End time: dim 15 jun 2025 04:31:04 CEST
Results:
============================================================
PYTORCH ENSEMBLE PIPELINE RESULTS
============================================================

OVERALL PERFORMANCE:
Overall Test R²: -0.0183
Overall Train R²: 0.0554
Overall RMSE: 0.120268
Overall MAE: 0.083055

WINDOW AVERAGES:
Average Test R²: -0.0300
Average Train R²: 0.0554
Average epochs: 34.5
Average features: 32.2
Feature reduction: 86.2%

R² BY WINDOW:
Window  1: Train R² = 0.0043 | Test R² = -0.0442
Window  2: Train R² = 0.0374 | Test R² = -0.0156
Window  3: Train R² = 0.1156 | Test R² = -0.0123
Window  4: Train R² = 0.0394 | Test R² = -0.0031
Window  5: Train R² = 0.0237 | Test R² = -0.0109
Window  6: Train R² = 0.0268 | Test R² = -0.0305
Window  7: Train R² = 0.0011 | Test R² = -0.0032
Window  8: Train R² = 0.0372 | Test R² = -0.0257
Window  9: Train R² = 0.0681 | Test R² = -0.0268
Window 10: Train R² = 0.1028 | Test R² = -0.0124
Window 11: Train R² = 0.0555 | Test R² = -0.0152
Window 12: Train R² = 0.0755 | Test R² = -0.0020
Window 13: Train R² = 0.0376 | Test R² = -0.0082
Window 14: Train R² = 0.0255 | Test R² = -0.0066
Window 15: Train R² = 0.0541 | Test R² = -0.0075
Window 16: Train R² = 0.0491 | Test R² = -0.0095
Window 17: Train R² = 0.0185 | Test R² = -0.0110
Window 18: Train R² = 0.0748 | Test R² = -0.0994
Window 19: Train R² = 0.0383 | Test R² = -0.0591
Window 20: Train R² = 0.0228 | Test R² = -0.0025
Window 21: Train R² = 0.0417 | Test R² = -0.0129
Window 22: Train R² = 0.0224 | Test R² = -0.0374
Window 23: Train R² = 0.0563 | Test R² = -0.0779
Window 24: Train R² = 0.0497 | Test R² = -0.0236
Window 25: Train R² = 0.0325 | Test R² = -0.0208
Window 26: Train R² = 0.0623 | Test R² = -0.0711
Window 27: Train R² = 0.1118 | Test R² = -0.0444
Window 28: Train R² = 0.0380 | Test R² = -0.0182
Window 29: Train R² = 0.0704 | Test R² = -0.0176
Window 30: Train R² = 0.0831 | Test R² = -0.0697
Window 31: Train R² = 0.0764 | Test R² = -0.0649
Window 32: Train R² = 0.0653 | Test R² = -0.0655
Window 33: Train R² = 0.0810 | Test R² = -0.0289
Window 34: Train R² = 0.0676 | Test R² = -0.0535
Window 35: Train R² = 0.0971 | Test R² = -0.0012
Window 36: Train R² = 0.1161 | Test R² = -0.0553
Window 37: Train R² = 0.0657 | Test R² = -0.0401
Window 38: Train R² = 0.0606 | Test R² = -0.0315

CONFIGURATION:
Training years: 20
Validation years: 5
Test years: 1
Architecture: [32, 16, 8]
Ensemble size: 1
Min features: 20
Max features: 50
Hyperparameter tuning: True
