Job ID: 2671741
Start time: sam 14 jun 2025 22:32:24 CEST
GPU: Tesla V100-PCIE-32GB
Starting pipeline...
================================================================================
SIMPLIFIED DEEP LEARNING PIPELINE - PYTORCH ENSEMBLE ONLY
================================================================================

1. LOADING DATA...
✓ Loaded 1,685,354 rows, 470 columns

2. CONFIGURATION:
✓ Date range: 1961-07-31 to 2024-12-31
✓ Target column: Excess_ret_target
✓ Train/Val/Test split: 20/5/1 years
✓ Feature selection: Random Forest (20-50 features)
✓ Model: PyTorch Ensemble (1 models)
✓ Architecture: [32, 16, 8]
✓ Hyperparameter tuning: Grid Search
✓ Grid search parameters: {'dropout_rate': [0.05, 0.1, 0.15]}

3. INITIALIZING PIPELINE...
Using device: cuda
Fixed architecture: [32, 16, 8]

4. STARTING SLIDING WINDOW ANALYSIS...
------------------------------------------------------------
Using 468 features (down from 470 total columns)
Period: 1961-07 to 2024-12
Training window: 20 years
Validation window: 5 years
Test window: 1 years

=== Window 1 ===
Train: 1961-07 to 1980-12 (69382 samples)
Val:   1981-01 to 1985-12 (41455 samples)
Test:  1986-01 to 1986-12 (10363 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (69382, 235)
Data cleaned successfully
→ Using min_samples_split=693, min_samples_leaf=346 for 69382 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 55505 training samples, 13877 validation samples
  20 features: Time-based R² = -0.0717
  25 features: Time-based R² = -0.0905
  30 features: Time-based R² = -0.0900
  35 features: Time-based R² = -0.1235
  40 features: Time-based R² = -0.1182
  45 features: Time-based R² = -0.1208
  50 features: Time-based R² = -0.1279
Optimal number of features: 20 (Time-based R² = -0.0717)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011512
  *** New best score: 0.011512

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010986
  *** New best score: 0.010986

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010479
  *** New best score: 0.010479

Grid search completed!
Best validation MSE: 0.010479
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0465 | R² TEST: 0.0148 ***
RMSE: 0.112231 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_01_training.png
  └─ Overfitting status: Minimal (Gap: 16.2%)

=== Window 2 ===
Train: 1962-01 to 1981-12 (75879 samples)
Val:   1982-01 to 1986-12 (45295 samples)
Test:  1987-01 to 1987-12 (11553 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (75879, 235)
Data cleaned successfully
→ Using min_samples_split=758, min_samples_leaf=379 for 75879 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 60703 training samples, 15176 validation samples
  20 features: Time-based R² = 0.0069
  25 features: Time-based R² = 0.0186
  30 features: Time-based R² = 0.0179
  35 features: Time-based R² = 0.0235
  40 features: Time-based R² = 0.0270
  45 features: Time-based R² = 0.0244
  50 features: Time-based R² = 0.0268
Optimal number of features: 40 (Time-based R² = 0.0270)
Selected 40 features from Random Forest
Added back 40 corresponding mask/flag features
Final feature count: 80
Feature reduction: 468 → 80 (17.1%)
Selected 80 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011341
  *** New best score: 0.011341

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011206
  *** New best score: 0.011206

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011364

Grid search completed!
Best validation MSE: 0.011206
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1932 | R² TEST: -0.0262 ***
RMSE: 0.126562 | Features: 80
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_02_training.png
  └─ Overfitting status: Moderate (Gap: 39.0%)

=== Window 3 ===
Train: 1963-01 to 1982-12 (82418 samples)
Val:   1983-01 to 1987-12 (48981 samples)
Test:  1988-01 to 1988-12 (12406 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (82418, 235)
Data cleaned successfully
→ Using min_samples_split=824, min_samples_leaf=412 for 82418 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 65934 training samples, 16484 validation samples
  20 features: Time-based R² = -0.0663
  25 features: Time-based R² = -0.0444
  30 features: Time-based R² = -0.0555
  35 features: Time-based R² = -0.0607
  40 features: Time-based R² = -0.0680
  45 features: Time-based R² = -0.0366
  50 features: Time-based R² = -0.0619
Optimal number of features: 45 (Time-based R² = -0.0366)
Selected 45 features from Random Forest
Added back 45 corresponding mask/flag features
Final feature count: 90
Feature reduction: 468 → 90 (19.2%)
Selected 90 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012794
  *** New best score: 0.012794

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012477
  *** New best score: 0.012477

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012121
  *** New best score: 0.012121

Grid search completed!
Best validation MSE: 0.012121
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1098 | R² TEST: -0.0703 ***
RMSE: 0.101716 | Features: 90
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_03_training.png
  └─ Overfitting status: Moderate (Gap: 32.7%)

=== Window 4 ===
Train: 1964-01 to 1983-12 (89067 samples)
Val:   1984-01 to 1988-12 (53019 samples)
Test:  1989-01 to 1989-12 (13048 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (89067, 235)
Data cleaned successfully
→ Using min_samples_split=890, min_samples_leaf=445 for 89067 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 71253 training samples, 17814 validation samples
  20 features: Time-based R² = -0.0879
  25 features: Time-based R² = -0.0394
  30 features: Time-based R² = -0.0358
  35 features: Time-based R² = -0.0287
  40 features: Time-based R² = -0.0378
  45 features: Time-based R² = -0.0329
  50 features: Time-based R² = -0.0168
Optimal number of features: 50 (Time-based R² = -0.0168)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012865
  *** New best score: 0.012865

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011873
  *** New best score: 0.011873

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011909

Grid search completed!
Best validation MSE: 0.011873
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1163 | R² TEST: -0.0587 ***
RMSE: 0.101102 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_04_training.png
  └─ Overfitting status: Moderate (Gap: 30.0%)

=== Window 5 ===
Train: 1965-01 to 1984-12 (96296 samples)
Val:   1985-01 to 1989-12 (56989 samples)
Test:  1990-01 to 1990-12 (13617 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (96296, 235)
Data cleaned successfully
→ Using min_samples_split=962, min_samples_leaf=481 for 96296 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 77036 training samples, 19260 validation samples
  20 features: Time-based R² = 0.0176
  25 features: Time-based R² = 0.0118
  30 features: Time-based R² = 0.0184
  35 features: Time-based R² = 0.0238
  40 features: Time-based R² = 0.0505
  45 features: Time-based R² = 0.0414
  50 features: Time-based R² = 0.0094
Optimal number of features: 40 (Time-based R² = 0.0505)
Selected 40 features from Random Forest
Added back 40 corresponding mask/flag features
Final feature count: 80
Feature reduction: 468 → 80 (17.1%)
Selected 80 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012145
  *** New best score: 0.012145

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011909
  *** New best score: 0.011909

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011792
  *** New best score: 0.011792

Grid search completed!
Best validation MSE: 0.011792
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0912 | R² TEST: 0.0446 ***
RMSE: 0.115370 | Features: 80
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_05_training.png
  └─ Overfitting status: Moderate (Gap: 22.8%)

=== Window 6 ===
Train: 1966-01 to 1985-12 (103898 samples)
Val:   1986-01 to 1990-12 (60987 samples)
Test:  1991-01 to 1991-12 (14227 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (103898, 235)
Data cleaned successfully
→ Using min_samples_split=1038, min_samples_leaf=519 for 103898 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 83118 training samples, 20780 validation samples
  20 features: Time-based R² = -0.1536
  25 features: Time-based R² = -0.1754
  30 features: Time-based R² = -0.1675
  35 features: Time-based R² = -0.1012
  40 features: Time-based R² = -0.1144
  45 features: Time-based R² = -0.0922
  50 features: Time-based R² = -0.0910
Optimal number of features: 50 (Time-based R² = -0.0910)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012842
  *** New best score: 0.012842

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012488
  *** New best score: 0.012488

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012504

Grid search completed!
Best validation MSE: 0.012488
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0931 | R² TEST: -0.0289 ***
RMSE: 0.118122 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_06_training.png
  └─ Overfitting status: Moderate (Gap: 33.8%)

=== Window 7 ===
Train: 1967-01 to 1986-12 (111671 samples)
Val:   1987-01 to 1991-12 (64851 samples)
Test:  1992-01 to 1992-12 (15449 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (111671, 235)
Data cleaned successfully
→ Using min_samples_split=1116, min_samples_leaf=558 for 111671 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 89336 training samples, 22335 validation samples
  20 features: Time-based R² = -0.0659
  25 features: Time-based R² = -0.0640
  30 features: Time-based R² = -0.0997
  35 features: Time-based R² = -0.0883
  40 features: Time-based R² = -0.1090
  45 features: Time-based R² = -0.0972
  50 features: Time-based R² = -0.0920
Optimal number of features: 25 (Time-based R² = -0.0640)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013896
  *** New best score: 0.013896

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.013161
  *** New best score: 0.013161

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012930
  *** New best score: 0.012930

Grid search completed!
Best validation MSE: 0.012930
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0461 | R² TEST: -0.0057 ***
RMSE: 0.106846 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_07_training.png
  └─ Overfitting status: Moderate (Gap: 25.7%)

=== Window 8 ===
Train: 1968-01 to 1987-12 (120310 samples)
Val:   1988-01 to 1992-12 (68747 samples)
Test:  1993-01 to 1993-12 (16943 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (120310, 235)
Data cleaned successfully
→ Using min_samples_split=1203, min_samples_leaf=601 for 120310 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 96248 training samples, 24062 validation samples
  20 features: Time-based R² = 0.0365
  25 features: Time-based R² = -0.0364
  30 features: Time-based R² = -0.0151
  35 features: Time-based R² = -0.0408
  40 features: Time-based R² = -0.0589
  45 features: Time-based R² = -0.1083
  50 features: Time-based R² = -0.0855
Optimal number of features: 20 (Time-based R² = 0.0365)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012868
  *** New best score: 0.012868

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012615
  *** New best score: 0.012615

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012397
  *** New best score: 0.012397

Grid search completed!
Best validation MSE: 0.012397
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0295 | R² TEST: -0.0237 ***
RMSE: 0.107617 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_08_training.png
  └─ Overfitting status: Minimal (Gap: 9.3%)

=== Window 9 ===
Train: 1969-01 to 1988-12 (129677 samples)
Val:   1989-01 to 1993-12 (73284 samples)
Test:  1994-01 to 1994-12 (18730 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (129677, 235)
Data cleaned successfully
→ Using min_samples_split=1296, min_samples_leaf=648 for 129677 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 103741 training samples, 25936 validation samples
  20 features: Time-based R² = -0.0607
  25 features: Time-based R² = 0.0147
  30 features: Time-based R² = -0.0018
  35 features: Time-based R² = -0.0052
  40 features: Time-based R² = -0.0948
  45 features: Time-based R² = -0.0941
  50 features: Time-based R² = -0.0954
Optimal number of features: 25 (Time-based R² = 0.0147)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012983
  *** New best score: 0.012983

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012561
  *** New best score: 0.012561

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012404
  *** New best score: 0.012404

Grid search completed!
Best validation MSE: 0.012404
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1217 | R² TEST: -0.0530 ***
RMSE: 0.102141 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_09_training.png
  └─ Overfitting status: Minimal (Gap: 18.7%)

=== Window 10 ===
Train: 1970-01 to 1989-12 (139494 samples)
Val:   1990-01 to 1994-12 (78966 samples)
Test:  1995-01 to 1995-12 (19857 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (139494, 235)
Data cleaned successfully
→ Using min_samples_split=1394, min_samples_leaf=697 for 139494 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 111595 training samples, 27899 validation samples
  20 features: Time-based R² = -0.1416
  25 features: Time-based R² = -0.1227
  30 features: Time-based R² = -0.1148
  35 features: Time-based R² = -0.1262
  40 features: Time-based R² = -0.1266
  45 features: Time-based R² = -0.1323
  50 features: Time-based R² = -0.1402
Optimal number of features: 30 (Time-based R² = -0.1148)
Selected 30 features from Random Forest
Added back 30 corresponding mask/flag features
Final feature count: 60
Feature reduction: 468 → 60 (12.8%)
Selected 60 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012824
  *** New best score: 0.012824

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012636
  *** New best score: 0.012636

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012502
  *** New best score: 0.012502

Grid search completed!
Best validation MSE: 0.012502
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1141 | R² TEST: -0.0123 ***
RMSE: 0.105949 | Features: 60
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_10_training.png
  └─ Overfitting status: Minimal (Gap: 18.4%)

=== Window 11 ===
Train: 1971-01 to 1990-12 (149676 samples)
Val:   1991-01 to 1995-12 (85206 samples)
Test:  1996-01 to 1996-12 (21557 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (149676, 235)
Data cleaned successfully
→ Using min_samples_split=1496, min_samples_leaf=748 for 149676 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 119740 training samples, 29936 validation samples
  20 features: Time-based R² = -0.0040
  25 features: Time-based R² = -0.0042
  30 features: Time-based R² = -0.0228
  35 features: Time-based R² = -0.0314
  40 features: Time-based R² = -0.0195
  45 features: Time-based R² = -0.0064
  50 features: Time-based R² = -0.0002
Optimal number of features: 50 (Time-based R² = -0.0002)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012157
  *** New best score: 0.012157

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012002
  *** New best score: 0.012002

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012114

Grid search completed!
Best validation MSE: 0.012002
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1219 | R² TEST: -0.0281 ***
RMSE: 0.112668 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_11_training.png
  └─ Overfitting status: Minimal (Gap: 13.9%)

=== Window 12 ===
Train: 1972-01 to 1991-12 (160253 samples)
Val:   1992-01 to 1996-12 (92536 samples)
Test:  1997-01 to 1997-12 (23201 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (160253, 235)
Data cleaned successfully
→ Using min_samples_split=1602, min_samples_leaf=801 for 160253 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 128202 training samples, 32051 validation samples
  20 features: Time-based R² = -0.0637
  25 features: Time-based R² = -0.0652
  30 features: Time-based R² = -0.0540
  35 features: Time-based R² = -0.0615
  40 features: Time-based R² = -0.0435
  45 features: Time-based R² = -0.0307
  50 features: Time-based R² = -0.0188
Optimal number of features: 50 (Time-based R² = -0.0188)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012249
  *** New best score: 0.012249

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011960
  *** New best score: 0.011960

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011988

Grid search completed!
Best validation MSE: 0.011960
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1554 | R² TEST: -0.0595 ***
RMSE: 0.120731 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_12_training.png
  └─ Overfitting status: Minimal (Gap: 8.8%)

=== Window 13 ===
Train: 1973-01 to 1992-12 (171807 samples)
Val:   1993-01 to 1997-12 (100288 samples)
Test:  1998-01 to 1998-12 (24897 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (171807, 235)
Data cleaned successfully
→ Using min_samples_split=1718, min_samples_leaf=859 for 171807 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 137445 training samples, 34362 validation samples
  20 features: Time-based R² = 0.0156
  25 features: Time-based R² = 0.0303
  30 features: Time-based R² = 0.0292
  35 features: Time-based R² = 0.0229
  40 features: Time-based R² = 0.0187
  45 features: Time-based R² = -0.0003
  50 features: Time-based R² = 0.0027
Optimal number of features: 25 (Time-based R² = 0.0303)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012666
  *** New best score: 0.012666

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012294
  *** New best score: 0.012294

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012195
  *** New best score: 0.012195

Grid search completed!
Best validation MSE: 0.012195
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0769 | R² TEST: -0.0096 ***
RMSE: 0.141921 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_13_training.png
  └─ Overfitting status: Minimal (Gap: 3.8%)

=== Window 14 ===
Train: 1974-01 to 1993-12 (184034 samples)
Val:   1994-01 to 1998-12 (108242 samples)
Test:  1999-01 to 1999-12 (25977 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (184034, 235)
Data cleaned successfully
→ Using min_samples_split=1840, min_samples_leaf=920 for 184034 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 147227 training samples, 36807 validation samples
  20 features: Time-based R² = -0.1517
  25 features: Time-based R² = -0.1379
  30 features: Time-based R² = -0.1474
  35 features: Time-based R² = -0.1433
  40 features: Time-based R² = -0.1663
  45 features: Time-based R² = -0.1704
  50 features: Time-based R² = -0.1661
Optimal number of features: 25 (Time-based R² = -0.1379)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.014515
  *** New best score: 0.014515

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.014749

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.014214
  *** New best score: 0.014214

Grid search completed!
Best validation MSE: 0.014214
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0740 | R² TEST: -0.0214 ***
RMSE: 0.141002 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_14_training.png
  └─ Overfitting status: Moderate (Gap: 21.1%)

=== Window 15 ===
Train: 1975-01 to 1994-12 (198005 samples)
Val:   1995-01 to 1999-12 (115489 samples)
Test:  2000-01 to 2000-12 (27607 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (198005, 235)
Data cleaned successfully
→ Using min_samples_split=1980, min_samples_leaf=990 for 198005 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 158404 training samples, 39601 validation samples
  20 features: Time-based R² = -0.2352
  25 features: Time-based R² = -0.2146
  30 features: Time-based R² = -0.1628
  35 features: Time-based R² = -0.1453
  40 features: Time-based R² = -0.1455
  45 features: Time-based R² = -0.1531
  50 features: Time-based R² = -0.1346
Optimal number of features: 50 (Time-based R² = -0.1346)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016492
  *** New best score: 0.016492

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015903
  *** New best score: 0.015903

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015637
  *** New best score: 0.015637

Grid search completed!
Best validation MSE: 0.015637
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0722 | R² TEST: -0.0442 ***
RMSE: 0.159141 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_15_training.png
  └─ Overfitting status: Moderate (Gap: 39.3%)

=== Window 16 ===
Train: 1976-01 to 1995-12 (213078 samples)
Val:   1996-01 to 2000-12 (123239 samples)
Test:  2001-01 to 2001-12 (28903 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (213078, 235)
Data cleaned successfully
→ Using min_samples_split=2130, min_samples_leaf=1065 for 213078 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 170462 training samples, 42616 validation samples
  20 features: Time-based R² = -0.0759
  25 features: Time-based R² = -0.0512
  30 features: Time-based R² = -0.0554
  35 features: Time-based R² = -0.0646
  40 features: Time-based R² = -0.0597
  45 features: Time-based R² = -0.0625
  50 features: Time-based R² = -0.0637
Optimal number of features: 25 (Time-based R² = -0.0512)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.018844
  *** New best score: 0.018844

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.018547
  *** New best score: 0.018547

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.018634

Grid search completed!
Best validation MSE: 0.018547
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0623 | R² TEST: -0.0080 ***
RMSE: 0.139709 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_16_training.png
  └─ Overfitting status: Severe (Gap: 60.1%)

=== Window 17 ===
Train: 1977-01 to 1996-12 (229767 samples)
Val:   1997-01 to 2001-12 (130585 samples)
Test:  2002-01 to 2002-12 (30051 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (229767, 235)
Data cleaned successfully
→ Using min_samples_split=2297, min_samples_leaf=1148 for 229767 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 183813 training samples, 45954 validation samples
  20 features: Time-based R² = -0.0439
  25 features: Time-based R² = -0.0495
  30 features: Time-based R² = -0.0286
  35 features: Time-based R² = -0.0194
  40 features: Time-based R² = -0.0034
  45 features: Time-based R² = -0.0060
  50 features: Time-based R² = -0.0049
Optimal number of features: 40 (Time-based R² = -0.0034)
Selected 40 features from Random Forest
Added back 40 corresponding mask/flag features
Final feature count: 80
Feature reduction: 468 → 80 (17.1%)
Selected 80 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.019834
  *** New best score: 0.019834

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.020612

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.019522
  *** New best score: 0.019522

Grid search completed!
Best validation MSE: 0.019522
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0415 | R² TEST: -0.0288 ***
RMSE: 0.132675 | Features: 80
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_17_training.png
  └─ Overfitting status: Severe (Gap: 68.3%)

=== Window 18 ===
Train: 1978-01 to 1997-12 (247961 samples)
Val:   1998-01 to 2002-12 (137435 samples)
Test:  2003-01 to 2003-12 (30738 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (247961, 235)
Data cleaned successfully
→ Using min_samples_split=2479, min_samples_leaf=1239 for 247961 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 198368 training samples, 49593 validation samples
  20 features: Time-based R² = -0.0101
  25 features: Time-based R² = -0.0453
  30 features: Time-based R² = -0.0250
  35 features: Time-based R² = -0.0533
  40 features: Time-based R² = -0.0382
  45 features: Time-based R² = -0.0379
  50 features: Time-based R² = -0.0418
Optimal number of features: 20 (Time-based R² = -0.0101)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.023134
  *** New best score: 0.023134

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.020383
  *** New best score: 0.020383

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.020365
  *** New best score: 0.020365

Grid search completed!
Best validation MSE: 0.020365
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0417 | R² TEST: -0.1088 ***
RMSE: 0.121543 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_18_training.png
  └─ Overfitting status: Severe (Gap: 88.4%)

=== Window 19 ===
Train: 1979-01 to 1998-12 (267762 samples)
Val:   1999-01 to 2003-12 (143276 samples)
Test:  2004-01 to 2004-12 (31983 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (267762, 235)
Data cleaned successfully
→ Using min_samples_split=2677, min_samples_leaf=1338 for 267762 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 214209 training samples, 53553 validation samples
  20 features: Time-based R² = -0.0921
  25 features: Time-based R² = -0.0849
  30 features: Time-based R² = -0.0841
  35 features: Time-based R² = -0.0893
  40 features: Time-based R² = -0.0650
  45 features: Time-based R² = -0.0294
  50 features: Time-based R² = -0.0443
Optimal number of features: 45 (Time-based R² = -0.0294)
Selected 45 features from Random Forest
Added back 45 corresponding mask/flag features
Final feature count: 90
Feature reduction: 468 → 90 (19.2%)
Selected 90 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.020737
  *** New best score: 0.020737

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.020042
  *** New best score: 0.020042

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.019903
  *** New best score: 0.019903

Grid search completed!
Best validation MSE: 0.019903
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0716 | R² TEST: -0.0155 ***
RMSE: 0.101982 | Features: 90
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_19_training.png
  └─ Overfitting status: Severe (Gap: 55.7%)

=== Window 20 ===
Train: 1980-01 to 1999-12 (288545 samples)
Val:   2000-01 to 2004-12 (149282 samples)
Test:  2005-01 to 2005-12 (33728 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (288545, 235)
Data cleaned successfully
→ Using min_samples_split=2885, min_samples_leaf=1442 for 288545 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 230836 training samples, 57709 validation samples
  20 features: Time-based R² = -0.0098
  25 features: Time-based R² = 0.0140
  30 features: Time-based R² = 0.0002
  35 features: Time-based R² = -0.0038
  40 features: Time-based R² = 0.0170
  45 features: Time-based R² = 0.0228
  50 features: Time-based R² = 0.0086
Optimal number of features: 45 (Time-based R² = 0.0228)
Selected 45 features from Random Forest
Added back 45 corresponding mask/flag features
Final feature count: 90
Feature reduction: 468 → 90 (19.2%)
Selected 90 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.018121
  *** New best score: 0.018121

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.018366

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.018062
  *** New best score: 0.018062

Grid search completed!
Best validation MSE: 0.018062
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0349 | R² TEST: -0.0309 ***
RMSE: 0.101713 | Features: 90
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_20_training.png
  └─ Overfitting status: Moderate (Gap: 28.6%)

=== Window 21 ===
Train: 1981-01 to 2000-12 (310887 samples)
Val:   2001-01 to 2005-12 (155403 samples)
Test:  2006-01 to 2006-12 (35831 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (310887, 235)
Data cleaned successfully
→ Using min_samples_split=3108, min_samples_leaf=1554 for 310887 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 248709 training samples, 62178 validation samples
  20 features: Time-based R² = 0.0019
  25 features: Time-based R² = 0.0063
  30 features: Time-based R² = -0.0207
  35 features: Time-based R² = -0.0095
  40 features: Time-based R² = -0.0066
  45 features: Time-based R² = 0.0009
  50 features: Time-based R² = -0.0514
Optimal number of features: 25 (Time-based R² = 0.0063)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.015985
  *** New best score: 0.015985

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015759
  *** New best score: 0.015759

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015703
  *** New best score: 0.015703

Grid search completed!
Best validation MSE: 0.015703
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0598 | R² TEST: -0.0242 ***
RMSE: 0.092569 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_21_training.png
  └─ Overfitting status: Minimal (Gap: -1.6%)

=== Window 22 ===
Train: 1982-01 to 2001-12 (333267 samples)
Val:   2002-01 to 2006-12 (162331 samples)
Test:  2007-01 to 2007-12 (39105 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (333267, 235)
Data cleaned successfully
→ Using min_samples_split=3332, min_samples_leaf=1666 for 333267 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 266613 training samples, 66654 validation samples
  20 features: Time-based R² = -0.0006
  25 features: Time-based R² = 0.0022
  30 features: Time-based R² = -0.0160
  35 features: Time-based R² = -0.0261
  40 features: Time-based R² = -0.0254
  45 features: Time-based R² = -0.0374
  50 features: Time-based R² = -0.0347
Optimal number of features: 25 (Time-based R² = 0.0022)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013815
  *** New best score: 0.013815

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.013001
  *** New best score: 0.013001

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012799
  *** New best score: 0.012799

Grid search completed!
Best validation MSE: 0.012799
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0500 | R² TEST: -0.0402 ***
RMSE: 0.102591 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_22_training.png
  └─ Overfitting status: Minimal (Gap: -21.4%)

=== Window 23 ===
Train: 1983-01 to 2002-12 (355451 samples)
Val:   2003-01 to 2007-12 (171385 samples)
Test:  2008-01 to 2008-12 (40868 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (355451, 235)
Data cleaned successfully
→ Using min_samples_split=3554, min_samples_leaf=1777 for 355451 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 284360 training samples, 71091 validation samples
  20 features: Time-based R² = -0.0349
  25 features: Time-based R² = -0.0668
  30 features: Time-based R² = -0.0727
  35 features: Time-based R² = -0.0796
  40 features: Time-based R² = -0.1288
  45 features: Time-based R² = -0.0727
  50 features: Time-based R² = -0.1714
Optimal number of features: 20 (Time-based R² = -0.0349)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011226
  *** New best score: 0.011226

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011289

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011284

Grid search completed!
Best validation MSE: 0.011226
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0627 | R² TEST: -0.0874 ***
RMSE: 0.155198 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_23_training.png
  └─ Overfitting status: Minimal (Gap: -30.0%)

=== Window 24 ===
Train: 1984-01 to 2003-12 (377821 samples)
Val:   2004-01 to 2008-12 (181515 samples)
Test:  2009-01 to 2009-12 (41319 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (377821, 235)
Data cleaned successfully
→ Using min_samples_split=3778, min_samples_leaf=1889 for 377821 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 302256 training samples, 75565 validation samples
  20 features: Time-based R² = -0.0504
  25 features: Time-based R² = -0.0401
  30 features: Time-based R² = -0.0252
  35 features: Time-based R² = -0.0416
  40 features: Time-based R² = -0.0387
  45 features: Time-based R² = -0.0289
  50 features: Time-based R² = -0.0117
Optimal number of features: 50 (Time-based R² = -0.0117)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013868
  *** New best score: 0.013868

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.013525
  *** New best score: 0.013525

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.013302
  *** New best score: 0.013302

Grid search completed!
Best validation MSE: 0.013302
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0491 | R² TEST: -0.0278 ***
RMSE: 0.149952 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_24_training.png
  └─ Overfitting status: Minimal (Gap: -20.7%)

=== Window 25 ===
Train: 1985-01 to 2004-12 (400726 samples)
Val:   2005-01 to 2009-12 (190851 samples)
Test:  2010-01 to 2010-12 (43249 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (400726, 235)
Data cleaned successfully
→ Using min_samples_split=4007, min_samples_leaf=2003 for 400726 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 320580 training samples, 80146 validation samples
  20 features: Time-based R² = -0.0198
  25 features: Time-based R² = -0.0114
  30 features: Time-based R² = -0.0006
  35 features: Time-based R² = 0.0091
  40 features: Time-based R² = -0.0350
  45 features: Time-based R² = -0.0824
  50 features: Time-based R² = -0.1290
Optimal number of features: 35 (Time-based R² = 0.0091)
Selected 35 features from Random Forest
Added back 35 corresponding mask/flag features
Final feature count: 70
Feature reduction: 468 → 70 (15.0%)
Selected 70 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016232
  *** New best score: 0.016232

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015675
  *** New best score: 0.015675

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015902

Grid search completed!
Best validation MSE: 0.015675
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0282 | R² TEST: -0.0217 ***
RMSE: 0.112507 | Features: 70
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_25_training.png
  └─ Overfitting status: Minimal (Gap: -4.2%)

=== Window 26 ===
Train: 1986-01 to 2005-12 (424835 samples)
Val:   2006-01 to 2010-12 (200372 samples)
Test:  2011-01 to 2011-12 (45523 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (424835, 235)
Data cleaned successfully
→ Using min_samples_split=4248, min_samples_leaf=2124 for 424835 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 339868 training samples, 84967 validation samples
  20 features: Time-based R² = -0.0401
  25 features: Time-based R² = -0.0735
  30 features: Time-based R² = -0.0833
  35 features: Time-based R² = -0.0618
  40 features: Time-based R² = -0.0590
  45 features: Time-based R² = -0.0142
  50 features: Time-based R² = -0.0885
Optimal number of features: 45 (Time-based R² = -0.0142)
Selected 45 features from Random Forest
Added back 45 corresponding mask/flag features
Final feature count: 90
Feature reduction: 468 → 90 (19.2%)
Selected 90 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016956
  *** New best score: 0.016956

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015974
  *** New best score: 0.015974

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.016008

Grid search completed!
Best validation MSE: 0.015974
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0756 | R² TEST: -0.1081 ***
RMSE: 0.117291 | Features: 90
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_26_training.png
  └─ Overfitting status: Minimal (Gap: 2.5%)

=== Window 27 ===
Train: 1987-01 to 2006-12 (450303 samples)
Val:   2007-01 to 2011-12 (210064 samples)
Test:  2012-01 to 2012-12 (47466 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (450303, 235)
Data cleaned successfully
→ Using min_samples_split=4503, min_samples_leaf=2251 for 450303 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 360242 training samples, 90061 validation samples
  20 features: Time-based R² = -0.0154
  25 features: Time-based R² = 0.0166
  30 features: Time-based R² = 0.0104
  35 features: Time-based R² = 0.0061
  40 features: Time-based R² = 0.0109
  45 features: Time-based R² = -0.0080
  50 features: Time-based R² = -0.0035
Optimal number of features: 25 (Time-based R² = 0.0166)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.017248
  *** New best score: 0.017248

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.016732
  *** New best score: 0.016732

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.016131
  *** New best score: 0.016131

Grid search completed!
Best validation MSE: 0.016131
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0511 | R² TEST: 0.0275 ***
RMSE: 0.094915 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_27_training.png
  └─ Overfitting status: Minimal (Gap: 9.1%)

=== Window 28 ===
Train: 1988-01 to 2007-12 (477855 samples)
Val:   2008-01 to 2012-12 (218425 samples)
Test:  2013-01 to 2013-12 (49097 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (477855, 235)
Data cleaned successfully
→ Using min_samples_split=4778, min_samples_leaf=2389 for 477855 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 382284 training samples, 95571 validation samples
  20 features: Time-based R² = 0.0049
  25 features: Time-based R² = 0.0028
  30 features: Time-based R² = 0.0001
  35 features: Time-based R² = -0.0076
  40 features: Time-based R² = -0.0116
  45 features: Time-based R² = 0.0018
  50 features: Time-based R² = -0.0040
Optimal number of features: 20 (Time-based R² = 0.0049)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.015914
  *** New best score: 0.015914

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015732
  *** New best score: 0.015732

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015620
  *** New best score: 0.015620

Grid search completed!
Best validation MSE: 0.015620
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0825 | R² TEST: -0.0268 ***
RMSE: 0.094640 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_28_training.png
  └─ Overfitting status: Minimal (Gap: 10.7%)

=== Window 29 ===
Train: 1989-01 to 2008-12 (506317 samples)
Val:   2009-01 to 2013-12 (226654 samples)
Test:  2014-01 to 2014-12 (52323 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (506317, 235)
Data cleaned successfully
→ Using min_samples_split=5063, min_samples_leaf=2531 for 506317 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 405053 training samples, 101264 validation samples
  20 features: Time-based R² = -0.0585
  25 features: Time-based R² = -0.0368
  30 features: Time-based R² = -0.0890
  35 features: Time-based R² = -0.0635
  40 features: Time-based R² = -0.0445
  45 features: Time-based R² = -0.0583
  50 features: Time-based R² = -0.0581
Optimal number of features: 25 (Time-based R² = -0.0368)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012844
  *** New best score: 0.012844

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012626
  *** New best score: 0.012626

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012748

Grid search completed!
Best validation MSE: 0.012626
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0872 | R² TEST: -0.0582 ***
RMSE: 0.095802 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_29_training.png
  └─ Overfitting status: Minimal (Gap: -9.7%)

=== Window 30 ===
Train: 1990-01 to 2009-12 (534588 samples)
Val:   2010-01 to 2014-12 (237658 samples)
Test:  2015-01 to 2015-12 (55529 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (534588, 235)
Data cleaned successfully
→ Using min_samples_split=5345, min_samples_leaf=2672 for 534588 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 427670 training samples, 106918 validation samples
  20 features: Time-based R² = 0.0110
  25 features: Time-based R² = 0.0022
  30 features: Time-based R² = -0.0246
  35 features: Time-based R² = -0.0364
  40 features: Time-based R² = -0.0150
  45 features: Time-based R² = -0.0658
  50 features: Time-based R² = -0.0664
Optimal number of features: 20 (Time-based R² = 0.0110)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011003
  *** New best score: 0.011003

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010830
  *** New best score: 0.010830

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010540
  *** New best score: 0.010540

Grid search completed!
Best validation MSE: 0.010540
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0824 | R² TEST: -0.0584 ***
RMSE: 0.108861 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_30_training.png
  └─ Overfitting status: Minimal (Gap: -30.6%)

=== Window 31 ===
Train: 1991-01 to 2010-12 (564220 samples)
Val:   2011-01 to 2015-12 (249938 samples)
Test:  2016-01 to 2016-12 (58133 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (564220, 235)
Data cleaned successfully
→ Using min_samples_split=5642, min_samples_leaf=2821 for 564220 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 451376 training samples, 112844 validation samples
  20 features: Time-based R² = -0.0725
  25 features: Time-based R² = -0.0487
  30 features: Time-based R² = -0.0177
  35 features: Time-based R² = -0.0552
  40 features: Time-based R² = -0.0744
  45 features: Time-based R² = -0.0796
  50 features: Time-based R² = -0.0779
Optimal number of features: 30 (Time-based R² = -0.0177)
Selected 30 features from Random Forest
Added back 30 corresponding mask/flag features
Final feature count: 60
Feature reduction: 468 → 60 (12.8%)
Selected 60 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010546
  *** New best score: 0.010546

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010359
  *** New best score: 0.010359

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010489

Grid search completed!
Best validation MSE: 0.010359
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1191 | R² TEST: -0.0246 ***
RMSE: 0.108342 | Features: 60
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_31_training.png
  └─ Overfitting status: Minimal (Gap: -28.5%)

=== Window 32 ===
Train: 1992-01 to 2011-12 (595516 samples)
Val:   2012-01 to 2016-12 (262548 samples)
Test:  2017-01 to 2017-12 (61214 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (595516, 235)
Data cleaned successfully
→ Using min_samples_split=5955, min_samples_leaf=2977 for 595516 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 476412 training samples, 119104 validation samples
  20 features: Time-based R² = -0.1259
  25 features: Time-based R² = -0.1238
  30 features: Time-based R² = -0.1094
  35 features: Time-based R² = -0.1059
  40 features: Time-based R² = -0.0932
  45 features: Time-based R² = -0.0976
  50 features: Time-based R² = -0.0912
Optimal number of features: 50 (Time-based R² = -0.0912)
Selected 50 features from Random Forest
Added back 50 corresponding mask/flag features
Final feature count: 100
Feature reduction: 468 → 100 (21.4%)
Selected 100 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010346
  *** New best score: 0.010346

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010438

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010409

Grid search completed!
Best validation MSE: 0.010346
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1658 | R² TEST: -0.1161 ***
RMSE: 0.098835 | Features: 100
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_32_training.png
  └─ Overfitting status: Minimal (Gap: -21.2%)

=== Window 33 ===
Train: 1993-01 to 2012-12 (627533 samples)
Val:   2013-01 to 2017-12 (276296 samples)
Test:  2018-01 to 2018-12 (65313 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (627533, 235)
Data cleaned successfully
→ Using min_samples_split=6275, min_samples_leaf=3137 for 627533 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 502026 training samples, 125507 validation samples
  20 features: Time-based R² = -0.0728
  25 features: Time-based R² = -0.0610
  30 features: Time-based R² = -0.0540
  35 features: Time-based R² = -0.0572
  40 features: Time-based R² = -0.0487
  45 features: Time-based R² = -0.0706
  50 features: Time-based R² = -0.0719
Optimal number of features: 40 (Time-based R² = -0.0487)
Selected 40 features from Random Forest
Added back 40 corresponding mask/flag features
Final feature count: 80
Feature reduction: 468 → 80 (17.1%)
Selected 80 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010866
  *** New best score: 0.010866

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010217
  *** New best score: 0.010217

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010207
  *** New best score: 0.010207

Grid search completed!
Best validation MSE: 0.010207
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1018 | R² TEST: -0.1112 ***
RMSE: 0.117320 | Features: 80
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_33_training.png
  └─ Overfitting status: Minimal (Gap: -26.3%)

=== Window 34 ===
Train: 1994-01 to 2013-12 (659687 samples)
Val:   2014-01 to 2018-12 (292512 samples)
Test:  2019-01 to 2019-12 (69500 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (659687, 235)
Data cleaned successfully
→ Using min_samples_split=6596, min_samples_leaf=3298 for 659687 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 527749 training samples, 131938 validation samples
  20 features: Time-based R² = -0.0522
  25 features: Time-based R² = -0.0797
  30 features: Time-based R² = -0.0755
  35 features: Time-based R² = -0.0799
  40 features: Time-based R² = -0.1139
  45 features: Time-based R² = -0.1244
  50 features: Time-based R² = -0.1179
Optimal number of features: 20 (Time-based R² = -0.0522)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011269
  *** New best score: 0.011269

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011106
  *** New best score: 0.011106

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010889
  *** New best score: 0.010889

Grid search completed!
Best validation MSE: 0.010889
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0789 | R² TEST: -0.0342 ***
RMSE: 0.109716 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_34_training.png
  └─ Overfitting status: Minimal (Gap: -20.7%)

=== Window 35 ===
Train: 1995-01 to 2014-12 (693280 samples)
Val:   2015-01 to 2019-12 (309689 samples)
Test:  2020-01 to 2020-12 (73804 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (693280, 235)
Data cleaned successfully
→ Using min_samples_split=6932, min_samples_leaf=3466 for 693280 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 554624 training samples, 138656 validation samples
  20 features: Time-based R² = -0.0208
  25 features: Time-based R² = -0.0252
  30 features: Time-based R² = -0.0217
  35 features: Time-based R² = -0.0007
  40 features: Time-based R² = -0.0158
  45 features: Time-based R² = -0.0143
  50 features: Time-based R² = -0.0451
Optimal number of features: 35 (Time-based R² = -0.0007)
Selected 35 features from Random Forest
Added back 35 corresponding mask/flag features
Final feature count: 70
Feature reduction: 468 → 70 (15.0%)
Selected 70 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012147
  *** New best score: 0.012147

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011678
  *** New best score: 0.011678

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011713

Grid search completed!
Best validation MSE: 0.011678
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1328 | R² TEST: -0.0709 ***
RMSE: 0.156754 | Features: 70
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_35_training.png
  └─ Overfitting status: Minimal (Gap: -8.2%)

=== Window 36 ===
Train: 1996-01 to 2015-12 (728952 samples)
Val:   2016-01 to 2020-12 (327964 samples)
Test:  2021-01 to 2021-12 (83029 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (728952, 235)
Data cleaned successfully
→ Using min_samples_split=7289, min_samples_leaf=3644 for 728952 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 583161 training samples, 145791 validation samples
  20 features: Time-based R² = -0.0001
  25 features: Time-based R² = -0.0259
  30 features: Time-based R² = -0.0230
  35 features: Time-based R² = -0.0205
  40 features: Time-based R² = -0.0130
  45 features: Time-based R² = -0.0184
  50 features: Time-based R² = -0.0234
Optimal number of features: 20 (Time-based R² = -0.0001)
Selected 20 features from Random Forest
Added back 20 corresponding mask/flag features
Final feature count: 40
Feature reduction: 468 → 40 (8.5%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.015283
  *** New best score: 0.015283

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.013892
  *** New best score: 0.013892

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.013883
  *** New best score: 0.013883

Grid search completed!
Best validation MSE: 0.013883
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0857 | R² TEST: -0.0735 ***
RMSE: 0.117899 | Features: 40
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_36_training.png
  └─ Overfitting status: Minimal (Gap: 9.1%)

=== Window 37 ===
Train: 1997-01 to 2016-12 (765528 samples)
Val:   2017-01 to 2021-12 (352860 samples)
Test:  2022-01 to 2022-12 (92602 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (765528, 235)
Data cleaned successfully
→ Using min_samples_split=7655, min_samples_leaf=3827 for 765528 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 612422 training samples, 153106 validation samples
  20 features: Time-based R² = -0.0910
  25 features: Time-based R² = -0.0783
  30 features: Time-based R² = -0.0862
  35 features: Time-based R² = -0.0487
  40 features: Time-based R² = -0.0674
  45 features: Time-based R² = -0.1293
  50 features: Time-based R² = -0.1216
Optimal number of features: 35 (Time-based R² = -0.0487)
Selected 35 features from Random Forest
Added back 35 corresponding mask/flag features
Final feature count: 70
Feature reduction: 468 → 70 (15.0%)
Selected 70 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.014992
  *** New best score: 0.014992

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.014531
  *** New best score: 0.014531

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.014358
  *** New best score: 0.014358

Grid search completed!
Best validation MSE: 0.014358
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1149 | R² TEST: -0.0525 ***
RMSE: 0.142031 | Features: 70
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_37_training.png
  └─ Overfitting status: Minimal (Gap: 11.5%)

=== Window 38 ===
Train: 1998-01 to 2017-12 (803541 samples)
Val:   2018-01 to 2022-12 (384248 samples)
Test:  2023-01 to 2023-12 (99011 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 234 regular features (excluding 233 mask/flag features)
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
  → Applied training winsorization limits to 234 regular features
  → Keeping 233 mask/flag features unchanged
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 235 regular features (excluding 233 mask/flag features)
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
  → Applied training normalization to 235 regular features
  → Combined 235 normalized features with 233 unchanged mask/flag features
Running feature selection on preprocessed data...
Total features available: 468
Excluding mask/flag features for selection: 235 base features from 468 total
Cleaning data - original shape: (803541, 235)
Data cleaned successfully
→ Using min_samples_split=8035, min_samples_leaf=4017 for 803541 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 642832 training samples, 160709 validation samples
  20 features: Time-based R² = -0.0464
  25 features: Time-based R² = -0.0415
  30 features: Time-based R² = -0.1272
  35 features: Time-based R² = -0.1258
  40 features: Time-based R² = -0.0746
  45 features: Time-based R² = -0.0999
  50 features: Time-based R² = -0.0990
Optimal number of features: 25 (Time-based R² = -0.0415)
Selected 25 features from Random Forest
Added back 25 corresponding mask/flag features
Final feature count: 50
Feature reduction: 468 → 50 (10.7%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016667
  *** New best score: 0.016667

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.016626
  *** New best score: 0.016626

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.016255
  *** New best score: 0.016255

Grid search completed!
Best validation MSE: 0.016255
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1405 | R² TEST: -0.0382 ***
RMSE: 0.131569 | Features: 50
  └─ Training plot saved: /home/kleymeno/RF_NN3/results/plots/individual_windows/window_38_training.png
  └─ Overfitting status: Moderate (Gap: 34.0%)

============================================================
FINAL RESULTS - PYTORCH ENSEMBLE
============================================================
Overall Test R²:     -0.0329
Overall Train R²:    0.0864
Overall RMSE:        0.1211
Average Window Test R²:  -0.0400
Average Window Train R²: 0.0864
Number of windows:   38
Average epochs:      32.7
Average features:    66.6
Feature reduction:   85.8%

R² BY WINDOW:
  Window  1: Train R² = 0.0465 | Test R² = 0.0148
  Window  2: Train R² = 0.1932 | Test R² = -0.0262
  Window  3: Train R² = 0.1098 | Test R² = -0.0703
  Window  4: Train R² = 0.1163 | Test R² = -0.0587
  Window  5: Train R² = 0.0912 | Test R² = 0.0446
  Window  6: Train R² = 0.0931 | Test R² = -0.0289
  Window  7: Train R² = 0.0461 | Test R² = -0.0057
  Window  8: Train R² = 0.0295 | Test R² = -0.0237
  Window  9: Train R² = 0.1217 | Test R² = -0.0530
  Window 10: Train R² = 0.1141 | Test R² = -0.0123
  Window 11: Train R² = 0.1219 | Test R² = -0.0281
  Window 12: Train R² = 0.1554 | Test R² = -0.0595
  Window 13: Train R² = 0.0769 | Test R² = -0.0096
  Window 14: Train R² = 0.0740 | Test R² = -0.0214
  Window 15: Train R² = 0.0722 | Test R² = -0.0442
  Window 16: Train R² = 0.0623 | Test R² = -0.0080
  Window 17: Train R² = 0.0415 | Test R² = -0.0288
  Window 18: Train R² = 0.0417 | Test R² = -0.1088
  Window 19: Train R² = 0.0716 | Test R² = -0.0155
  Window 20: Train R² = 0.0349 | Test R² = -0.0309
  Window 21: Train R² = 0.0598 | Test R² = -0.0242
  Window 22: Train R² = 0.0500 | Test R² = -0.0402
  Window 23: Train R² = 0.0627 | Test R² = -0.0874
  Window 24: Train R² = 0.0491 | Test R² = -0.0278
  Window 25: Train R² = 0.0282 | Test R² = -0.0217
  Window 26: Train R² = 0.0756 | Test R² = -0.1081
  Window 27: Train R² = 0.0511 | Test R² = 0.0275
  Window 28: Train R² = 0.0825 | Test R² = -0.0268
  Window 29: Train R² = 0.0872 | Test R² = -0.0582
  Window 30: Train R² = 0.0824 | Test R² = -0.0584
  Window 31: Train R² = 0.1191 | Test R² = -0.0246
  Window 32: Train R² = 0.1658 | Test R² = -0.1161
  Window 33: Train R² = 0.1018 | Test R² = -0.1112
  Window 34: Train R² = 0.0789 | Test R² = -0.0342
  Window 35: Train R² = 0.1328 | Test R² = -0.0709
  Window 36: Train R² = 0.0857 | Test R² = -0.0735
  Window 37: Train R² = 0.1149 | Test R² = -0.0525
  Window 38: Train R² = 0.1405 | Test R² = -0.0382
============================================================

5. SAVING RESULTS...
✓ Including permno data in predictions file
✓ Predictions saved: /home/kleymeno/RF_NN3/results/predictions.csv
✓ Window metrics saved: /home/kleymeno/RF_NN3/results/window_metrics.csv
✓ Feature evolution saved: /home/kleymeno/RF_NN3/results/feature_evolution.csv
✓ Overall results saved: /home/kleymeno/RF_NN3/results/overall_results.txt

6. GENERATING FINAL PLOTS...

=== GENERATING SUMMARY PLOTS ===
Main summary saved to: /home/kleymeno/RF_NN3/results/plots/main_summary.png
R² bar charts saved to: /home/kleymeno/RF_NN3/results/plots/r2_bar_charts.png
Performance analysis saved to: /home/kleymeno/RF_NN3/results/plots/performance_analysis.png
Predictions vs actual plot saved to: /home/kleymeno/RF_NN3/results/plots/predictions_vs_actual.png
✓ All plots saved to: /home/kleymeno/RF_NN3/results/plots

============================================================
FINAL SUMMARY
============================================================
Total windows processed: 38
Overall Test R²: -0.0329
Overall Train R²: 0.0864
Overall RMSE: 0.121131
Average features per window: 66.6
Average epochs per window: 32.7

Best Test R² Window: 0.0446
Worst Test R² Window: -0.1161
R² Standard Deviation: 0.0359

Results directory: /home/kleymeno/RF_NN3/results
Plots directory: /home/kleymeno/RF_NN3/results/plots

================================================================================
ANALYSIS COMPLETE!
================================================================================
End time: dim 15 jun 2025 09:36:41 CEST
Results:
============================================================
PYTORCH ENSEMBLE PIPELINE RESULTS
============================================================

OVERALL PERFORMANCE:
Overall Test R²: -0.0329
Overall Train R²: 0.0864
Overall RMSE: 0.121131
Overall MAE: 0.083873

WINDOW AVERAGES:
Average Test R²: -0.0400
Average Train R²: 0.0864
Average epochs: 32.7
Average features: 66.6
Feature reduction: 85.8%

R² BY WINDOW:
Window  1: Train R² = 0.0465 | Test R² = 0.0148
Window  2: Train R² = 0.1932 | Test R² = -0.0262
Window  3: Train R² = 0.1098 | Test R² = -0.0703
Window  4: Train R² = 0.1163 | Test R² = -0.0587
Window  5: Train R² = 0.0912 | Test R² = 0.0446
Window  6: Train R² = 0.0931 | Test R² = -0.0289
Window  7: Train R² = 0.0461 | Test R² = -0.0057
Window  8: Train R² = 0.0295 | Test R² = -0.0237
Window  9: Train R² = 0.1217 | Test R² = -0.0530
Window 10: Train R² = 0.1141 | Test R² = -0.0123
Window 11: Train R² = 0.1219 | Test R² = -0.0281
Window 12: Train R² = 0.1554 | Test R² = -0.0595
Window 13: Train R² = 0.0769 | Test R² = -0.0096
Window 14: Train R² = 0.0740 | Test R² = -0.0214
Window 15: Train R² = 0.0722 | Test R² = -0.0442
Window 16: Train R² = 0.0623 | Test R² = -0.0080
Window 17: Train R² = 0.0415 | Test R² = -0.0288
Window 18: Train R² = 0.0417 | Test R² = -0.1088
Window 19: Train R² = 0.0716 | Test R² = -0.0155
Window 20: Train R² = 0.0349 | Test R² = -0.0309
Window 21: Train R² = 0.0598 | Test R² = -0.0242
Window 22: Train R² = 0.0500 | Test R² = -0.0402
Window 23: Train R² = 0.0627 | Test R² = -0.0874
Window 24: Train R² = 0.0491 | Test R² = -0.0278
Window 25: Train R² = 0.0282 | Test R² = -0.0217
Window 26: Train R² = 0.0756 | Test R² = -0.1081
Window 27: Train R² = 0.0511 | Test R² = 0.0275
Window 28: Train R² = 0.0825 | Test R² = -0.0268
Window 29: Train R² = 0.0872 | Test R² = -0.0582
Window 30: Train R² = 0.0824 | Test R² = -0.0584
Window 31: Train R² = 0.1191 | Test R² = -0.0246
Window 32: Train R² = 0.1658 | Test R² = -0.1161
Window 33: Train R² = 0.1018 | Test R² = -0.1112
Window 34: Train R² = 0.0789 | Test R² = -0.0342
Window 35: Train R² = 0.1328 | Test R² = -0.0709
Window 36: Train R² = 0.0857 | Test R² = -0.0735
Window 37: Train R² = 0.1149 | Test R² = -0.0525
Window 38: Train R² = 0.1405 | Test R² = -0.0382

CONFIGURATION:
Training years: 20
Validation years: 5
Test years: 1
Architecture: [32, 16, 8]
Ensemble size: 1
Min features: 20
Max features: 50
Hyperparameter tuning: True
