Job ID: 2671684
Start time: sam 14 jun 2025 18:11:47 CEST
GPU: Tesla V100-PCIE-32GB
Starting pipeline...
================================================================================
SIMPLIFIED DEEP LEARNING PIPELINE - PYTORCH ENSEMBLE ONLY
================================================================================

1. LOADING DATA...
✓ Loaded 1,685,354 rows, 470 columns

2. CONFIGURATION:
✓ Date range: 1961-07-31 to 2024-12-31
✓ Target column: Excess_ret_target
✓ Train/Val/Test split: 20/5/1 years
✓ Feature selection: Random Forest (20-50 features)
✓ Model: PyTorch Ensemble (1 models)
✓ Architecture: [32, 16, 8]
✓ Hyperparameter tuning: Grid Search
✓ Grid search parameters: {'dropout_rate': [0.05, 0.1, 0.15]}

3. INITIALIZING PIPELINE...
Using device: cuda
Fixed architecture: [32, 16, 8]

4. STARTING SLIDING WINDOW ANALYSIS...
------------------------------------------------------------
Excluded 234 mask/flag features
Using 234 features (down from 470 total columns)
Period: 1961-07 to 2024-12
Training window: 20 years
Validation window: 5 years
Test window: 1 years

=== Window 1 ===
Train: 1961-07 to 1980-12 (69382 samples)
Val:   1981-01 to 1985-12 (41455 samples)
Test:  1986-01 to 1986-12 (10363 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (69382, 234)
Data cleaned successfully
→ Using min_samples_split=693, min_samples_leaf=346 for 69382 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 55505 training samples, 13877 validation samples
  20 features: Time-based R² = -0.0166
  25 features: Time-based R² = -0.0760
  30 features: Time-based R² = -0.0882
  35 features: Time-based R² = -0.1052
  40 features: Time-based R² = -0.1112
  45 features: Time-based R² = -0.1293
  50 features: Time-based R² = -0.1023
Optimal number of features: 20 (Time-based R² = -0.0166)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010982
  *** New best score: 0.010982

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010672
  *** New best score: 0.010672

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010836

Grid search completed!
Best validation MSE: 0.010672
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1144 | R² TEST: -0.0868 ***
RMSE: 0.117876 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_01_training.png
  └─ Overfitting status: Moderate (Gap: 25.7%)

=== Window 2 ===
Train: 1962-01 to 1981-12 (75879 samples)
Val:   1982-01 to 1986-12 (45295 samples)
Test:  1987-01 to 1987-12 (11553 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (75879, 234)
Data cleaned successfully
→ Using min_samples_split=758, min_samples_leaf=379 for 75879 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 60703 training samples, 15176 validation samples
  20 features: Time-based R² = -0.0324
  25 features: Time-based R² = -0.0564
  30 features: Time-based R² = -0.0058
  35 features: Time-based R² = -0.0054
  40 features: Time-based R² = 0.0032
  45 features: Time-based R² = 0.0201
  50 features: Time-based R² = 0.0220
Optimal number of features: 50 (Time-based R² = 0.0220)
Selected 50 optimal features from Random Forest
Feature reduction: 234 → 50 (21.4%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011459
  *** New best score: 0.011459

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011122
  *** New best score: 0.011122

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011189

Grid search completed!
Best validation MSE: 0.011122
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1675 | R² TEST: 0.0400 ***
RMSE: 0.122412 | Features: 50
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_02_training.png
  └─ Overfitting status: Moderate (Gap: 37.0%)

=== Window 3 ===
Train: 1963-01 to 1982-12 (82418 samples)
Val:   1983-01 to 1987-12 (48981 samples)
Test:  1988-01 to 1988-12 (12406 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (82418, 234)
Data cleaned successfully
→ Using min_samples_split=824, min_samples_leaf=412 for 82418 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 65934 training samples, 16484 validation samples
  20 features: Time-based R² = -0.1123
  25 features: Time-based R² = -0.1126
  30 features: Time-based R² = -0.0704
  35 features: Time-based R² = -0.0520
  40 features: Time-based R² = -0.0529
  45 features: Time-based R² = -0.0739
  50 features: Time-based R² = -0.0679
Optimal number of features: 35 (Time-based R² = -0.0520)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011990
  *** New best score: 0.011990

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012360

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012100

Grid search completed!
Best validation MSE: 0.011990
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.2252 | R² TEST: -0.0810 ***
RMSE: 0.102222 | Features: 35
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_03_training.png
  └─ Overfitting status: Severe (Gap: 51.9%)

=== Window 4 ===
Train: 1964-01 to 1983-12 (89067 samples)
Val:   1984-01 to 1988-12 (53019 samples)
Test:  1989-01 to 1989-12 (13048 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (89067, 234)
Data cleaned successfully
→ Using min_samples_split=890, min_samples_leaf=445 for 89067 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 71253 training samples, 17814 validation samples
  20 features: Time-based R² = -0.0675
  25 features: Time-based R² = -0.0436
  30 features: Time-based R² = 0.0061
  35 features: Time-based R² = -0.0401
  40 features: Time-based R² = -0.0201
  45 features: Time-based R² = -0.0243
  50 features: Time-based R² = -0.0274
Optimal number of features: 30 (Time-based R² = 0.0061)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011837
  *** New best score: 0.011837

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012134

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011762
  *** New best score: 0.011762

Grid search completed!
Best validation MSE: 0.011762
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1458 | R² TEST: -0.0576 ***
RMSE: 0.101047 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_04_training.png
  └─ Overfitting status: Moderate (Gap: 32.3%)

=== Window 5 ===
Train: 1965-01 to 1984-12 (96296 samples)
Val:   1985-01 to 1989-12 (56989 samples)
Test:  1990-01 to 1990-12 (13617 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (96296, 234)
Data cleaned successfully
→ Using min_samples_split=962, min_samples_leaf=481 for 96296 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 77036 training samples, 19260 validation samples
  20 features: Time-based R² = -0.0252
  25 features: Time-based R² = -0.0047
  30 features: Time-based R² = -0.0027
  35 features: Time-based R² = 0.0193
  40 features: Time-based R² = 0.0304
  45 features: Time-based R² = 0.0213
  50 features: Time-based R² = 0.0171
Optimal number of features: 40 (Time-based R² = 0.0304)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012078
  *** New best score: 0.012078

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011461
  *** New best score: 0.011461

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011444
  *** New best score: 0.011444

Grid search completed!
Best validation MSE: 0.011444
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0800 | R² TEST: 0.0274 ***
RMSE: 0.116404 | Features: 40
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_05_training.png
  └─ Overfitting status: Moderate (Gap: 23.0%)

=== Window 6 ===
Train: 1966-01 to 1985-12 (103898 samples)
Val:   1986-01 to 1990-12 (60987 samples)
Test:  1991-01 to 1991-12 (14227 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (103898, 234)
Data cleaned successfully
→ Using min_samples_split=1038, min_samples_leaf=519 for 103898 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 83118 training samples, 20780 validation samples
  20 features: Time-based R² = -0.0456
  25 features: Time-based R² = -0.0776
  30 features: Time-based R² = -0.1023
  35 features: Time-based R² = -0.1266
  40 features: Time-based R² = -0.1002
  45 features: Time-based R² = -0.0690
  50 features: Time-based R² = -0.0547
Optimal number of features: 20 (Time-based R² = -0.0456)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012834
  *** New best score: 0.012834

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012484
  *** New best score: 0.012484

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012371
  *** New best score: 0.012371

Grid search completed!
Best validation MSE: 0.012371
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0842 | R² TEST: -0.0274 ***
RMSE: 0.118039 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_06_training.png
  └─ Overfitting status: Moderate (Gap: 29.8%)

=== Window 7 ===
Train: 1967-01 to 1986-12 (111671 samples)
Val:   1987-01 to 1991-12 (64851 samples)
Test:  1992-01 to 1992-12 (15449 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (111671, 234)
Data cleaned successfully
→ Using min_samples_split=1116, min_samples_leaf=558 for 111671 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 89336 training samples, 22335 validation samples
  20 features: Time-based R² = 0.0106
  25 features: Time-based R² = -0.0496
  30 features: Time-based R² = -0.0368
  35 features: Time-based R² = -0.0446
  40 features: Time-based R² = -0.0523
  45 features: Time-based R² = -0.0588
  50 features: Time-based R² = -0.1924
Optimal number of features: 20 (Time-based R² = 0.0106)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.014057
  *** New best score: 0.014057

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.013141
  *** New best score: 0.013141

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012839
  *** New best score: 0.012839

Grid search completed!
Best validation MSE: 0.012839
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0905 | R² TEST: -0.0157 ***
RMSE: 0.107377 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_07_training.png
  └─ Overfitting status: Moderate (Gap: 31.6%)

=== Window 8 ===
Train: 1968-01 to 1987-12 (120310 samples)
Val:   1988-01 to 1992-12 (68747 samples)
Test:  1993-01 to 1993-12 (16943 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (120310, 234)
Data cleaned successfully
→ Using min_samples_split=1203, min_samples_leaf=601 for 120310 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 96248 training samples, 24062 validation samples
  20 features: Time-based R² = -0.0877
  25 features: Time-based R² = -0.0241
  30 features: Time-based R² = -0.0134
  35 features: Time-based R² = -0.0007
  40 features: Time-based R² = -0.0271
  45 features: Time-based R² = -0.0412
  50 features: Time-based R² = -0.0400
Optimal number of features: 35 (Time-based R² = -0.0007)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013084
  *** New best score: 0.013084

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012671
  *** New best score: 0.012671

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012308
  *** New best score: 0.012308

Grid search completed!
Best validation MSE: 0.012308
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0617 | R² TEST: -0.0169 ***
RMSE: 0.107262 | Features: 35
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_08_training.png
  └─ Overfitting status: Minimal (Gap: 11.2%)

=== Window 9 ===
Train: 1969-01 to 1988-12 (129677 samples)
Val:   1989-01 to 1993-12 (73284 samples)
Test:  1994-01 to 1994-12 (18730 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (129677, 234)
Data cleaned successfully
→ Using min_samples_split=1296, min_samples_leaf=648 for 129677 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 103741 training samples, 25936 validation samples
  20 features: Time-based R² = -0.0428
  25 features: Time-based R² = -0.0412
  30 features: Time-based R² = -0.0861
  35 features: Time-based R² = -0.0906
  40 features: Time-based R² = -0.0389
  45 features: Time-based R² = -0.0445
  50 features: Time-based R² = -0.0802
Optimal number of features: 40 (Time-based R² = -0.0389)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013528
  *** New best score: 0.013528

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012891
  *** New best score: 0.012891

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012534
  *** New best score: 0.012534

Grid search completed!
Best validation MSE: 0.012534
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0694 | R² TEST: -0.0619 ***
RMSE: 0.102570 | Features: 40
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_09_training.png
  └─ Overfitting status: Minimal (Gap: 15.1%)

=== Window 10 ===
Train: 1970-01 to 1989-12 (139494 samples)
Val:   1990-01 to 1994-12 (78966 samples)
Test:  1995-01 to 1995-12 (19857 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (139494, 234)
Data cleaned successfully
→ Using min_samples_split=1394, min_samples_leaf=697 for 139494 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 111595 training samples, 27899 validation samples
  20 features: Time-based R² = -0.0878
  25 features: Time-based R² = -0.0992
  30 features: Time-based R² = -0.1009
  35 features: Time-based R² = -0.0955
  40 features: Time-based R² = -0.0835
  45 features: Time-based R² = -0.0721
  50 features: Time-based R² = -0.1062
Optimal number of features: 45 (Time-based R² = -0.0721)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012961
  *** New best score: 0.012961

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012687
  *** New best score: 0.012687

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012440
  *** New best score: 0.012440

Grid search completed!
Best validation MSE: 0.012440
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1069 | R² TEST: -0.0180 ***
RMSE: 0.106245 | Features: 45
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_10_training.png
  └─ Overfitting status: Minimal (Gap: 15.9%)

=== Window 11 ===
Train: 1971-01 to 1990-12 (149676 samples)
Val:   1991-01 to 1995-12 (85206 samples)
Test:  1996-01 to 1996-12 (21557 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (149676, 234)
Data cleaned successfully
→ Using min_samples_split=1496, min_samples_leaf=748 for 149676 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 119740 training samples, 29936 validation samples
  20 features: Time-based R² = 0.0711
  25 features: Time-based R² = 0.0372
  30 features: Time-based R² = 0.0481
  35 features: Time-based R² = 0.0340
  40 features: Time-based R² = 0.0536
  45 features: Time-based R² = 0.0445
  50 features: Time-based R² = 0.0407
Optimal number of features: 20 (Time-based R² = 0.0711)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012379
  *** New best score: 0.012379

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012180
  *** New best score: 0.012180

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012053
  *** New best score: 0.012053

Grid search completed!
Best validation MSE: 0.012053
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0674 | R² TEST: -0.0938 ***
RMSE: 0.116213 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_11_training.png
  └─ Overfitting status: Minimal (Gap: 5.7%)

=== Window 12 ===
Train: 1972-01 to 1991-12 (160253 samples)
Val:   1992-01 to 1996-12 (92536 samples)
Test:  1997-01 to 1997-12 (23201 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (160253, 234)
Data cleaned successfully
→ Using min_samples_split=1602, min_samples_leaf=801 for 160253 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 128202 training samples, 32051 validation samples
  20 features: Time-based R² = 0.0814
  25 features: Time-based R² = 0.0790
  30 features: Time-based R² = 0.0821
  35 features: Time-based R² = 0.0791
  40 features: Time-based R² = 0.0718
  45 features: Time-based R² = 0.0740
  50 features: Time-based R² = 0.0760
Optimal number of features: 30 (Time-based R² = 0.0821)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012022
  *** New best score: 0.012022

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012186

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012122

Grid search completed!
Best validation MSE: 0.012022
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1242 | R² TEST: -0.0936 ***
RMSE: 0.122653 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_12_training.png
  └─ Overfitting status: Minimal (Gap: 7.3%)

=== Window 13 ===
Train: 1973-01 to 1992-12 (171807 samples)
Val:   1993-01 to 1997-12 (100288 samples)
Test:  1998-01 to 1998-12 (24897 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (171807, 234)
Data cleaned successfully
→ Using min_samples_split=1718, min_samples_leaf=859 for 171807 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 137445 training samples, 34362 validation samples
  20 features: Time-based R² = -0.0389
  25 features: Time-based R² = 0.0055
  30 features: Time-based R² = -0.0090
  35 features: Time-based R² = 0.0035
  40 features: Time-based R² = -0.0054
  45 features: Time-based R² = -0.0054
  50 features: Time-based R² = -0.0083
Optimal number of features: 25 (Time-based R² = 0.0055)
Selected 25 optimal features from Random Forest
Feature reduction: 234 → 25 (10.7%)
Selected 25 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013177
  *** New best score: 0.013177

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012479
  *** New best score: 0.012479

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012334
  *** New best score: 0.012334

Grid search completed!
Best validation MSE: 0.012334
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0776 | R² TEST: -0.0928 ***
RMSE: 0.147657 | Features: 25
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_13_training.png
  └─ Overfitting status: Minimal (Gap: 4.3%)

=== Window 14 ===
Train: 1974-01 to 1993-12 (184034 samples)
Val:   1994-01 to 1998-12 (108242 samples)
Test:  1999-01 to 1999-12 (25977 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (184034, 234)
Data cleaned successfully
→ Using min_samples_split=1840, min_samples_leaf=920 for 184034 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 147227 training samples, 36807 validation samples
  20 features: Time-based R² = -0.0415
  25 features: Time-based R² = -0.0311
  30 features: Time-based R² = -0.0183
  35 features: Time-based R² = -0.0083
  40 features: Time-based R² = -0.0144
  45 features: Time-based R² = -0.0157
  50 features: Time-based R² = -0.0096
Optimal number of features: 35 (Time-based R² = -0.0083)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.014297
  *** New best score: 0.014297

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.014290
  *** New best score: 0.014290

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.014117
  *** New best score: 0.014117

Grid search completed!
Best validation MSE: 0.014117
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0325 | R² TEST: -0.0011 ***
RMSE: 0.139594 | Features: 35
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_14_training.png
  └─ Overfitting status: Minimal (Gap: 19.5%)

=== Window 15 ===
Train: 1975-01 to 1994-12 (198005 samples)
Val:   1995-01 to 1999-12 (115489 samples)
Test:  2000-01 to 2000-12 (27607 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (198005, 234)
Data cleaned successfully
→ Using min_samples_split=1980, min_samples_leaf=990 for 198005 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 158404 training samples, 39601 validation samples
  20 features: Time-based R² = -0.0573
  25 features: Time-based R² = -0.0549
  30 features: Time-based R² = -0.0685
  35 features: Time-based R² = -0.0524
  40 features: Time-based R² = -0.0282
  45 features: Time-based R² = -0.0573
  50 features: Time-based R² = -0.0573
Optimal number of features: 40 (Time-based R² = -0.0282)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.015984
  *** New best score: 0.015984

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.016571

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015869
  *** New best score: 0.015869

Grid search completed!
Best validation MSE: 0.015869
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0257 | R² TEST: -0.0096 ***
RMSE: 0.156483 | Features: 40
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_15_training.png
  └─ Overfitting status: Moderate (Gap: 34.3%)

=== Window 16 ===
Train: 1976-01 to 1995-12 (213078 samples)
Val:   1996-01 to 2000-12 (123239 samples)
Test:  2001-01 to 2001-12 (28903 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (213078, 234)
Data cleaned successfully
→ Using min_samples_split=2130, min_samples_leaf=1065 for 213078 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 170462 training samples, 42616 validation samples
  20 features: Time-based R² = -0.0357
  25 features: Time-based R² = -0.0341
  30 features: Time-based R² = -0.0282
  35 features: Time-based R² = -0.0403
  40 features: Time-based R² = -0.0664
  45 features: Time-based R² = -0.0547
  50 features: Time-based R² = -0.0641
Optimal number of features: 30 (Time-based R² = -0.0282)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.018825
  *** New best score: 0.018825

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.018449
  *** New best score: 0.018449

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.018723

Grid search completed!
Best validation MSE: 0.018449
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0615 | R² TEST: 0.0219 ***
RMSE: 0.137622 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_16_training.png
  └─ Overfitting status: Severe (Gap: 77.8%)

=== Window 17 ===
Train: 1977-01 to 1996-12 (229767 samples)
Val:   1997-01 to 2001-12 (130585 samples)
Test:  2002-01 to 2002-12 (30051 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (229767, 234)
Data cleaned successfully
→ Using min_samples_split=2297, min_samples_leaf=1148 for 229767 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 183813 training samples, 45954 validation samples
  20 features: Time-based R² = -0.0370
  25 features: Time-based R² = -0.0318
  30 features: Time-based R² = -0.0257
  35 features: Time-based R² = -0.0089
  40 features: Time-based R² = -0.0050
  45 features: Time-based R² = 0.0016
  50 features: Time-based R² = -0.0047
Optimal number of features: 45 (Time-based R² = 0.0016)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.020184
  *** New best score: 0.020184

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.020355

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.019634
  *** New best score: 0.019634

Grid search completed!
Best validation MSE: 0.019634
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1037 | R² TEST: -0.0187 ***
RMSE: 0.132026 | Features: 45
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_17_training.png
  └─ Overfitting status: Severe (Gap: 70.1%)

=== Window 18 ===
Train: 1978-01 to 1997-12 (247961 samples)
Val:   1998-01 to 2002-12 (137435 samples)
Test:  2003-01 to 2003-12 (30738 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (247961, 234)
Data cleaned successfully
→ Using min_samples_split=2479, min_samples_leaf=1239 for 247961 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 198368 training samples, 49593 validation samples
  20 features: Time-based R² = -0.0443
  25 features: Time-based R² = -0.0441
  30 features: Time-based R² = -0.0412
  35 features: Time-based R² = -0.0649
  40 features: Time-based R² = -0.0628
  45 features: Time-based R² = -0.0810
  50 features: Time-based R² = -0.0579
Optimal number of features: 30 (Time-based R² = -0.0412)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.021199
  *** New best score: 0.021199

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.019940
  *** New best score: 0.019940

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.020440

Grid search completed!
Best validation MSE: 0.019940
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0705 | R² TEST: -0.0768 ***
RMSE: 0.119774 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_18_training.png
  └─ Overfitting status: Severe (Gap: 73.1%)

=== Window 19 ===
Train: 1979-01 to 1998-12 (267762 samples)
Val:   1999-01 to 2003-12 (143276 samples)
Test:  2004-01 to 2004-12 (31983 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (267762, 234)
Data cleaned successfully
→ Using min_samples_split=2677, min_samples_leaf=1338 for 267762 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 214209 training samples, 53553 validation samples
  20 features: Time-based R² = 0.0243
  25 features: Time-based R² = 0.0295
  30 features: Time-based R² = 0.0323
  35 features: Time-based R² = 0.0266
  40 features: Time-based R² = 0.0200
  45 features: Time-based R² = 0.0200
  50 features: Time-based R² = 0.0226
Optimal number of features: 30 (Time-based R² = 0.0323)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.020944
  *** New best score: 0.020944

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.019978
  *** New best score: 0.019978

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.019785
  *** New best score: 0.019785

Grid search completed!
Best validation MSE: 0.019785
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0333 | R² TEST: -0.0163 ***
RMSE: 0.102025 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_19_training.png
  └─ Overfitting status: Moderate (Gap: 48.7%)

=== Window 20 ===
Train: 1980-01 to 1999-12 (288545 samples)
Val:   2000-01 to 2004-12 (149282 samples)
Test:  2005-01 to 2005-12 (33728 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (288545, 234)
Data cleaned successfully
→ Using min_samples_split=2885, min_samples_leaf=1442 for 288545 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 230836 training samples, 57709 validation samples
  20 features: Time-based R² = 0.0302
  25 features: Time-based R² = 0.0264
  30 features: Time-based R² = -0.0795
  35 features: Time-based R² = -0.0341
  40 features: Time-based R² = -0.0358
  45 features: Time-based R² = -0.0317
  50 features: Time-based R² = -0.0341
Optimal number of features: 20 (Time-based R² = 0.0302)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.017993
  *** New best score: 0.017993

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.018502

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.018187

Grid search completed!
Best validation MSE: 0.017993
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0814 | R² TEST: -0.0164 ***
RMSE: 0.100999 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_20_training.png
  └─ Overfitting status: Moderate (Gap: 41.2%)

=== Window 21 ===
Train: 1981-01 to 2000-12 (310887 samples)
Val:   2001-01 to 2005-12 (155403 samples)
Test:  2006-01 to 2006-12 (35831 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (310887, 234)
Data cleaned successfully
→ Using min_samples_split=3108, min_samples_leaf=1554 for 310887 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 248709 training samples, 62178 validation samples
  20 features: Time-based R² = 0.0008
  25 features: Time-based R² = -0.0023
  30 features: Time-based R² = 0.0020
  35 features: Time-based R² = 0.0014
  40 features: Time-based R² = 0.0003
  45 features: Time-based R² = 0.0066
  50 features: Time-based R² = 0.0065
Optimal number of features: 45 (Time-based R² = 0.0066)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016004
  *** New best score: 0.016004

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015631
  *** New best score: 0.015631

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015262
  *** New best score: 0.015262

Grid search completed!
Best validation MSE: 0.015262
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0689 | R² TEST: -0.0124 ***
RMSE: 0.092035 | Features: 45
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_21_training.png
  └─ Overfitting status: Minimal (Gap: 0.6%)

=== Window 22 ===
Train: 1982-01 to 2001-12 (333267 samples)
Val:   2002-01 to 2006-12 (162331 samples)
Test:  2007-01 to 2007-12 (39105 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (333267, 234)
Data cleaned successfully
→ Using min_samples_split=3332, min_samples_leaf=1666 for 333267 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 266613 training samples, 66654 validation samples
  20 features: Time-based R² = -0.0190
  25 features: Time-based R² = -0.0356
  30 features: Time-based R² = -0.0117
  35 features: Time-based R² = -0.0265
  40 features: Time-based R² = -0.0239
  45 features: Time-based R² = -0.0296
  50 features: Time-based R² = -0.0205
Optimal number of features: 30 (Time-based R² = -0.0117)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012877
  *** New best score: 0.012877

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012681
  *** New best score: 0.012681

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.012780

Grid search completed!
Best validation MSE: 0.012681
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0714 | R² TEST: -0.0344 ***
RMSE: 0.102307 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_22_training.png
  └─ Overfitting status: Minimal (Gap: -19.7%)

=== Window 23 ===
Train: 1983-01 to 2002-12 (355451 samples)
Val:   2003-01 to 2007-12 (171385 samples)
Test:  2008-01 to 2008-12 (40868 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (355451, 234)
Data cleaned successfully
→ Using min_samples_split=3554, min_samples_leaf=1777 for 355451 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 284360 training samples, 71091 validation samples
  20 features: Time-based R² = -0.1145
  25 features: Time-based R² = -0.1332
  30 features: Time-based R² = -0.1332
  35 features: Time-based R² = -0.1394
  40 features: Time-based R² = -0.1575
  45 features: Time-based R² = -0.1376
  50 features: Time-based R² = -0.1423
Optimal number of features: 20 (Time-based R² = -0.1145)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011305
  *** New best score: 0.011305

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011204
  *** New best score: 0.011204

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011136
  *** New best score: 0.011136

Grid search completed!
Best validation MSE: 0.011136
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0766 | R² TEST: -0.0383 ***
RMSE: 0.151652 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_23_training.png
  └─ Overfitting status: Minimal (Gap: -30.1%)

=== Window 24 ===
Train: 1984-01 to 2003-12 (377821 samples)
Val:   2004-01 to 2008-12 (181515 samples)
Test:  2009-01 to 2009-12 (41319 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (377821, 234)
Data cleaned successfully
→ Using min_samples_split=3778, min_samples_leaf=1889 for 377821 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 302256 training samples, 75565 validation samples
  20 features: Time-based R² = -0.0402
  25 features: Time-based R² = -0.0286
  30 features: Time-based R² = -0.0507
  35 features: Time-based R² = -0.0283
  40 features: Time-based R² = -0.0405
  45 features: Time-based R² = -0.0671
  50 features: Time-based R² = -0.0496
Optimal number of features: 35 (Time-based R² = -0.0283)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.013269
  *** New best score: 0.013269

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.013857

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.013290

Grid search completed!
Best validation MSE: 0.013269
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1022 | R² TEST: -0.1749 ***
RMSE: 0.160329 | Features: 35
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_24_training.png
  └─ Overfitting status: Minimal (Gap: -11.0%)

=== Window 25 ===
Train: 1985-01 to 2004-12 (400726 samples)
Val:   2005-01 to 2009-12 (190851 samples)
Test:  2010-01 to 2010-12 (43249 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (400726, 234)
Data cleaned successfully
→ Using min_samples_split=4007, min_samples_leaf=2003 for 400726 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 320580 training samples, 80146 validation samples
  20 features: Time-based R² = -0.0372
  25 features: Time-based R² = -0.0267
  30 features: Time-based R² = -0.0105
  35 features: Time-based R² = -0.0177
  40 features: Time-based R² = -0.0240
  45 features: Time-based R² = -0.0301
  50 features: Time-based R² = -0.0441
Optimal number of features: 30 (Time-based R² = -0.0105)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016037
  *** New best score: 0.016037

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015652
  *** New best score: 0.015652

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015653

Grid search completed!
Best validation MSE: 0.015652
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0633 | R² TEST: -0.0574 ***
RMSE: 0.114456 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_25_training.png
  └─ Overfitting status: Minimal (Gap: 2.8%)

=== Window 26 ===
Train: 1986-01 to 2005-12 (424835 samples)
Val:   2006-01 to 2010-12 (200372 samples)
Test:  2011-01 to 2011-12 (45523 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (424835, 234)
Data cleaned successfully
→ Using min_samples_split=4248, min_samples_leaf=2124 for 424835 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 339868 training samples, 84967 validation samples
  20 features: Time-based R² = -0.1108
  25 features: Time-based R² = -0.1507
  30 features: Time-based R² = -0.1404
  35 features: Time-based R² = -0.0999
  40 features: Time-based R² = -0.0202
  45 features: Time-based R² = -0.0305
  50 features: Time-based R² = -0.0304
Optimal number of features: 40 (Time-based R² = -0.0202)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016277
  *** New best score: 0.016277

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015662
  *** New best score: 0.015662

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.015675

Grid search completed!
Best validation MSE: 0.015662
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0987 | R² TEST: -0.1918 ***
RMSE: 0.121643 | Features: 40
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_26_training.png
  └─ Overfitting status: Minimal (Gap: 1.3%)

=== Window 27 ===
Train: 1987-01 to 2006-12 (450303 samples)
Val:   2007-01 to 2011-12 (210064 samples)
Test:  2012-01 to 2012-12 (47466 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (450303, 234)
Data cleaned successfully
→ Using min_samples_split=4503, min_samples_leaf=2251 for 450303 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 360242 training samples, 90061 validation samples
  20 features: Time-based R² = -0.1028
  25 features: Time-based R² = -0.0443
  30 features: Time-based R² = -0.0384
  35 features: Time-based R² = -0.0331
  40 features: Time-based R² = -0.0311
  45 features: Time-based R² = -0.0161
  50 features: Time-based R² = -0.0248
Optimal number of features: 45 (Time-based R² = -0.0161)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016377
  *** New best score: 0.016377

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.016507

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.016765

Grid search completed!
Best validation MSE: 0.016377
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1063 | R² TEST: -0.0048 ***
RMSE: 0.096482 | Features: 45
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_27_training.png
  └─ Overfitting status: Moderate (Gap: 21.8%)

=== Window 28 ===
Train: 1988-01 to 2007-12 (477855 samples)
Val:   2008-01 to 2012-12 (218425 samples)
Test:  2013-01 to 2013-12 (49097 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (477855, 234)
Data cleaned successfully
→ Using min_samples_split=4778, min_samples_leaf=2389 for 477855 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 382284 training samples, 95571 validation samples
  20 features: Time-based R² = -0.0436
  25 features: Time-based R² = -0.0635
  30 features: Time-based R² = -0.0361
  35 features: Time-based R² = -0.0398
  40 features: Time-based R² = -0.0345
  45 features: Time-based R² = -0.0256
  50 features: Time-based R² = -0.0415
Optimal number of features: 45 (Time-based R² = -0.0256)
Selected 45 optimal features from Random Forest
Feature reduction: 234 → 45 (19.2%)
Selected 45 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.016945
  *** New best score: 0.016945

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.016084
  *** New best score: 0.016084

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.016199

Grid search completed!
Best validation MSE: 0.016084
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0933 | R² TEST: -0.0483 ***
RMSE: 0.095623 | Features: 45
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_28_training.png
  └─ Overfitting status: Minimal (Gap: 15.3%)

=== Window 29 ===
Train: 1989-01 to 2008-12 (506317 samples)
Val:   2009-01 to 2013-12 (226654 samples)
Test:  2014-01 to 2014-12 (52323 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (506317, 234)
Data cleaned successfully
→ Using min_samples_split=5063, min_samples_leaf=2531 for 506317 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 405053 training samples, 101264 validation samples
  20 features: Time-based R² = -0.0425
  25 features: Time-based R² = -0.0261
  30 features: Time-based R² = -0.0100
  35 features: Time-based R² = -0.0061
  40 features: Time-based R² = -0.0244
  45 features: Time-based R² = -0.1304
  50 features: Time-based R² = -0.1313
Optimal number of features: 35 (Time-based R² = -0.0061)
Selected 35 optimal features from Random Forest
Feature reduction: 234 → 35 (15.0%)
Selected 35 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.012761
  *** New best score: 0.012761

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.012860

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.013497

Grid search completed!
Best validation MSE: 0.012761
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1066 | R² TEST: -0.1008 ***
RMSE: 0.097709 | Features: 35
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_29_training.png
  └─ Overfitting status: Minimal (Gap: -10.9%)

=== Window 30 ===
Train: 1990-01 to 2009-12 (534588 samples)
Val:   2010-01 to 2014-12 (237658 samples)
Test:  2015-01 to 2015-12 (55529 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (534588, 234)
Data cleaned successfully
→ Using min_samples_split=5345, min_samples_leaf=2672 for 534588 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 427670 training samples, 106918 validation samples
  20 features: Time-based R² = -0.0194
  25 features: Time-based R² = -0.0239
  30 features: Time-based R² = -0.0089
  35 features: Time-based R² = -0.0203
  40 features: Time-based R² = -0.0077
  45 features: Time-based R² = -0.0255
  50 features: Time-based R² = -0.0116
Optimal number of features: 40 (Time-based R² = -0.0077)
Selected 40 optimal features from Random Forest
Feature reduction: 234 → 40 (17.1%)
Selected 40 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010850
  *** New best score: 0.010850

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010964

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010979

Grid search completed!
Best validation MSE: 0.010850
Best parameters: {'dropout_rate': 0.05}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1421 | R² TEST: -0.0688 ***
RMSE: 0.109391 | Features: 40
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_30_training.png
  └─ Overfitting status: Minimal (Gap: -27.8%)

=== Window 31 ===
Train: 1991-01 to 2010-12 (564220 samples)
Val:   2011-01 to 2015-12 (249938 samples)
Test:  2016-01 to 2016-12 (58133 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (564220, 234)
Data cleaned successfully
→ Using min_samples_split=5642, min_samples_leaf=2821 for 564220 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 451376 training samples, 112844 validation samples
  20 features: Time-based R² = -0.0539
  25 features: Time-based R² = -0.0332
  30 features: Time-based R² = -0.2124
  35 features: Time-based R² = -0.1962
  40 features: Time-based R² = -0.1702
  45 features: Time-based R² = -0.1346
  50 features: Time-based R² = -0.1439
Optimal number of features: 25 (Time-based R² = -0.0332)
Selected 25 optimal features from Random Forest
Feature reduction: 234 → 25 (10.7%)
Selected 25 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010799
  *** New best score: 0.010799

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010566
  *** New best score: 0.010566

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010348
  *** New best score: 0.010348

Grid search completed!
Best validation MSE: 0.010348
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0686 | R² TEST: -0.0212 ***
RMSE: 0.108167 | Features: 25
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_31_training.png
  └─ Overfitting status: Minimal (Gap: -31.6%)

=== Window 32 ===
Train: 1992-01 to 2011-12 (595516 samples)
Val:   2012-01 to 2016-12 (262548 samples)
Test:  2017-01 to 2017-12 (61214 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (595516, 234)
Data cleaned successfully
→ Using min_samples_split=5955, min_samples_leaf=2977 for 595516 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 476412 training samples, 119104 validation samples
  20 features: Time-based R² = -0.0107
  25 features: Time-based R² = -0.0047
  30 features: Time-based R² = -0.4285
  35 features: Time-based R² = -0.1084
  40 features: Time-based R² = -0.1448
  45 features: Time-based R² = -0.1142
  50 features: Time-based R² = -0.0996
Optimal number of features: 25 (Time-based R² = -0.0047)
Selected 25 optimal features from Random Forest
Feature reduction: 234 → 25 (10.7%)
Selected 25 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011000
  *** New best score: 0.011000

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.010685
  *** New best score: 0.010685

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010356
  *** New best score: 0.010356

Grid search completed!
Best validation MSE: 0.010356
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0809 | R² TEST: -0.0627 ***
RMSE: 0.096440 | Features: 25
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_32_training.png
  └─ Overfitting status: Minimal (Gap: -29.1%)

=== Window 33 ===
Train: 1993-01 to 2012-12 (627533 samples)
Val:   2013-01 to 2017-12 (276296 samples)
Test:  2018-01 to 2018-12 (65313 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (627533, 234)
Data cleaned successfully
→ Using min_samples_split=6275, min_samples_leaf=3137 for 627533 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 502026 training samples, 125507 validation samples
  20 features: Time-based R² = -0.0124
  25 features: Time-based R² = -0.0105
  30 features: Time-based R² = -0.0269
  35 features: Time-based R² = -0.0274
  40 features: Time-based R² = -0.0260
  45 features: Time-based R² = -0.0212
  50 features: Time-based R² = -0.0009
Optimal number of features: 50 (Time-based R² = -0.0009)
Selected 50 optimal features from Random Forest
Feature reduction: 234 → 50 (21.4%)
Selected 50 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010683
  *** New best score: 0.010683

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.009931
  *** New best score: 0.009931

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010124

Grid search completed!
Best validation MSE: 0.009931
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1197 | R² TEST: -0.0787 ***
RMSE: 0.115588 | Features: 50
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_33_training.png
  └─ Overfitting status: Minimal (Gap: -27.4%)

=== Window 34 ===
Train: 1994-01 to 2013-12 (659687 samples)
Val:   2014-01 to 2018-12 (292512 samples)
Test:  2019-01 to 2019-12 (69500 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (659687, 234)
Data cleaned successfully
→ Using min_samples_split=6596, min_samples_leaf=3298 for 659687 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 527749 training samples, 131938 validation samples
  20 features: Time-based R² = 0.0203
  25 features: Time-based R² = 0.0130
  30 features: Time-based R² = -0.0351
  35 features: Time-based R² = -0.0761
  40 features: Time-based R² = -0.0692
  45 features: Time-based R² = -0.1069
  50 features: Time-based R² = -0.0674
Optimal number of features: 20 (Time-based R² = 0.0203)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.010933
  *** New best score: 0.010933

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011026

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.010713
  *** New best score: 0.010713

Grid search completed!
Best validation MSE: 0.010713
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0979 | R² TEST: -0.0512 ***
RMSE: 0.110610 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_34_training.png
  └─ Overfitting status: Minimal (Gap: -21.4%)

=== Window 35 ===
Train: 1995-01 to 2014-12 (693280 samples)
Val:   2015-01 to 2019-12 (309689 samples)
Test:  2020-01 to 2020-12 (73804 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (693280, 234)
Data cleaned successfully
→ Using min_samples_split=6932, min_samples_leaf=3466 for 693280 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 554624 training samples, 138656 validation samples
  20 features: Time-based R² = -0.0772
  25 features: Time-based R² = -0.0358
  30 features: Time-based R² = -0.0272
  35 features: Time-based R² = -0.0572
  40 features: Time-based R² = -0.0560
  45 features: Time-based R² = -0.0529
  50 features: Time-based R² = -0.0315
Optimal number of features: 30 (Time-based R² = -0.0272)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.011991
  *** New best score: 0.011991

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.011512
  *** New best score: 0.011512

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.011511
  *** New best score: 0.011511

Grid search completed!
Best validation MSE: 0.011511
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0969 | R² TEST: -0.0398 ***
RMSE: 0.154459 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_35_training.png
  └─ Overfitting status: Minimal (Gap: -12.9%)

=== Window 36 ===
Train: 1996-01 to 2015-12 (728952 samples)
Val:   2016-01 to 2020-12 (327964 samples)
Test:  2021-01 to 2021-12 (83029 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (728952, 234)
Data cleaned successfully
→ Using min_samples_split=7289, min_samples_leaf=3644 for 728952 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 583161 training samples, 145791 validation samples
  20 features: Time-based R² = -0.0079
  25 features: Time-based R² = -0.0016
  30 features: Time-based R² = 0.0019
  35 features: Time-based R² = -0.0113
  40 features: Time-based R² = -0.0119
  45 features: Time-based R² = -0.0319
  50 features: Time-based R² = -0.0199
Optimal number of features: 30 (Time-based R² = 0.0019)
Selected 30 optimal features from Random Forest
Feature reduction: 234 → 30 (12.8%)
Selected 30 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.014792
  *** New best score: 0.014792

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.014114
  *** New best score: 0.014114

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.013662
  *** New best score: 0.013662

Grid search completed!
Best validation MSE: 0.013662
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0988 | R² TEST: -0.0489 ***
RMSE: 0.116543 | Features: 30
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_36_training.png
  └─ Overfitting status: Minimal (Gap: 6.2%)

=== Window 37 ===
Train: 1997-01 to 2016-12 (765528 samples)
Val:   2017-01 to 2021-12 (352860 samples)
Test:  2022-01 to 2022-12 (92602 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (765528, 234)
Data cleaned successfully
→ Using min_samples_split=7655, min_samples_leaf=3827 for 765528 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 612422 training samples, 153106 validation samples
  20 features: Time-based R² = -0.0370
  25 features: Time-based R² = -0.0378
  30 features: Time-based R² = -0.0954
  35 features: Time-based R² = -0.0632
  40 features: Time-based R² = -0.0598
  45 features: Time-based R² = -0.0481
  50 features: Time-based R² = -0.0426
Optimal number of features: 20 (Time-based R² = -0.0370)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.014780
  *** New best score: 0.014780

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.014099
  *** New best score: 0.014099

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.014056
  *** New best score: 0.014056

Grid search completed!
Best validation MSE: 0.014056
Best parameters: {'dropout_rate': 0.15}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.1153 | R² TEST: -0.0559 ***
RMSE: 0.142263 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_37_training.png
  └─ Overfitting status: Minimal (Gap: 10.5%)

=== Window 38 ===
Train: 1998-01 to 2017-12 (803541 samples)
Val:   2018-01 to 2022-12 (384248 samples)
Test:  2023-01 to 2023-12 (99011 samples)
Applying winsorization (fit on train, apply to all)...
  → Fitted winsorization on 233 features using training data
  → Applied training winsorization limits to 233 features
  → Applied training winsorization limits to 233 features
Applying normalization (fit on train, apply to all)...
  → Fitted normalization on 234 features using training data
  → Applied training normalization to 234 features
  → Applied training normalization to 234 features
Running feature selection on preprocessed data...
Total features available: 234
Cleaning data - original shape: (803541, 234)
Data cleaned successfully
→ Using min_samples_split=8035, min_samples_leaf=4017 for 803541 rows
Finding optimal number of features between 20 and 50...
Using time-based validation to respect panel structure...
Time-based split: 642832 training samples, 160709 validation samples
  20 features: Time-based R² = -0.0035
  25 features: Time-based R² = -0.0164
  30 features: Time-based R² = -0.0175
  35 features: Time-based R² = -0.0714
  40 features: Time-based R² = -0.0696
  45 features: Time-based R² = -0.0616
  50 features: Time-based R² = -0.0951
Optimal number of features: 20 (Time-based R² = -0.0035)
Selected 20 optimal features from Random Forest
Feature reduction: 234 → 20 (8.5%)
Selected 20 features
Running grid search hyperparameter optimization...
Starting grid search hyperparameter optimization...
Testing 3 parameter combinations:
  dropout_rate: [0.05, 0.1, 0.15]

Combination 1/3: {'dropout_rate': 0.05}
Training ensemble model 1/1
  Validation MSE: 0.017502
  *** New best score: 0.017502

Combination 2/3: {'dropout_rate': 0.1}
Training ensemble model 1/1
  Validation MSE: 0.015995
  *** New best score: 0.015995

Combination 3/3: {'dropout_rate': 0.15}
Training ensemble model 1/1
  Validation MSE: 0.016443

Grid search completed!
Best validation MSE: 0.015995
Best parameters: {'dropout_rate': 0.1}
Grid search complete
Training PyTorch ensemble model...
PyTorch Ensemble initialized - Device: cuda
Architecture: [32, 16, 8] | Estimators: 1
Training ensemble model 1/1
*** R² TRAIN: 0.0800 | R² TEST: -0.0695 ***
RMSE: 0.133542 | Features: 20
  └─ Training plot saved: /home/tgraber/RF_NN3/results/plots/individual_windows/window_38_training.png
  └─ Overfitting status: Moderate (Gap: 29.9%)

============================================================
FINAL RESULTS - PYTORCH ENSEMBLE
============================================================
Overall Test R²:     -0.0395
Overall Train R²:    0.0924
Overall RMSE:        0.1215
Average Window Test R²:  -0.0488
Average Window Train R²: 0.0924
Number of windows:   38
Average epochs:      34.3
Average features:    32.2
Feature reduction:   86.2%

R² BY WINDOW:
  Window  1: Train R² = 0.1144 | Test R² = -0.0868
  Window  2: Train R² = 0.1675 | Test R² = 0.0400
  Window  3: Train R² = 0.2252 | Test R² = -0.0810
  Window  4: Train R² = 0.1458 | Test R² = -0.0576
  Window  5: Train R² = 0.0800 | Test R² = 0.0274
  Window  6: Train R² = 0.0842 | Test R² = -0.0274
  Window  7: Train R² = 0.0905 | Test R² = -0.0157
  Window  8: Train R² = 0.0617 | Test R² = -0.0169
  Window  9: Train R² = 0.0694 | Test R² = -0.0619
  Window 10: Train R² = 0.1069 | Test R² = -0.0180
  Window 11: Train R² = 0.0674 | Test R² = -0.0938
  Window 12: Train R² = 0.1242 | Test R² = -0.0936
  Window 13: Train R² = 0.0776 | Test R² = -0.0928
  Window 14: Train R² = 0.0325 | Test R² = -0.0011
  Window 15: Train R² = 0.0257 | Test R² = -0.0096
  Window 16: Train R² = 0.0615 | Test R² = 0.0219
  Window 17: Train R² = 0.1037 | Test R² = -0.0187
  Window 18: Train R² = 0.0705 | Test R² = -0.0768
  Window 19: Train R² = 0.0333 | Test R² = -0.0163
  Window 20: Train R² = 0.0814 | Test R² = -0.0164
  Window 21: Train R² = 0.0689 | Test R² = -0.0124
  Window 22: Train R² = 0.0714 | Test R² = -0.0344
  Window 23: Train R² = 0.0766 | Test R² = -0.0383
  Window 24: Train R² = 0.1022 | Test R² = -0.1749
  Window 25: Train R² = 0.0633 | Test R² = -0.0574
  Window 26: Train R² = 0.0987 | Test R² = -0.1918
  Window 27: Train R² = 0.1063 | Test R² = -0.0048
  Window 28: Train R² = 0.0933 | Test R² = -0.0483
  Window 29: Train R² = 0.1066 | Test R² = -0.1008
  Window 30: Train R² = 0.1421 | Test R² = -0.0688
  Window 31: Train R² = 0.0686 | Test R² = -0.0212
  Window 32: Train R² = 0.0809 | Test R² = -0.0627
  Window 33: Train R² = 0.1197 | Test R² = -0.0787
  Window 34: Train R² = 0.0979 | Test R² = -0.0512
  Window 35: Train R² = 0.0969 | Test R² = -0.0398
  Window 36: Train R² = 0.0988 | Test R² = -0.0489
  Window 37: Train R² = 0.1153 | Test R² = -0.0559
  Window 38: Train R² = 0.0800 | Test R² = -0.0695
============================================================

5. SAVING RESULTS...
✓ Including permno data in predictions file
✓ Predictions saved: /home/tgraber/RF_NN3/results/predictions.csv
✓ Window metrics saved: /home/tgraber/RF_NN3/results/window_metrics.csv
✓ Feature evolution saved: /home/tgraber/RF_NN3/results/feature_evolution.csv
✓ Overall results saved: /home/tgraber/RF_NN3/results/overall_results.txt

6. GENERATING FINAL PLOTS...

=== GENERATING SUMMARY PLOTS ===
Main summary saved to: /home/tgraber/RF_NN3/results/plots/main_summary.png
R² bar charts saved to: /home/tgraber/RF_NN3/results/plots/r2_bar_charts.png
Performance analysis saved to: /home/tgraber/RF_NN3/results/plots/performance_analysis.png
Predictions vs actual plot saved to: /home/tgraber/RF_NN3/results/plots/predictions_vs_actual.png
✓ All plots saved to: /home/tgraber/RF_NN3/results/plots

============================================================
FINAL SUMMARY
============================================================
Total windows processed: 38
Overall Test R²: -0.0395
Overall Train R²: 0.0924
Overall RMSE: 0.121519
Average features per window: 32.2
Average epochs per window: 34.3

Best Test R² Window: 0.0400
Worst Test R² Window: -0.1918
R² Standard Deviation: 0.0470

Results directory: /home/tgraber/RF_NN3/results
Plots directory: /home/tgraber/RF_NN3/results/plots

================================================================================
ANALYSIS COMPLETE!
================================================================================
End time: dim 15 jun 2025 05:50:26 CEST
Results:
============================================================
PYTORCH ENSEMBLE PIPELINE RESULTS
============================================================

OVERALL PERFORMANCE:
Overall Test R²: -0.0395
Overall Train R²: 0.0924
Overall RMSE: 0.121519
Overall MAE: 0.084258

WINDOW AVERAGES:
Average Test R²: -0.0488
Average Train R²: 0.0924
Average epochs: 34.3
Average features: 32.2
Feature reduction: 86.2%

R² BY WINDOW:
Window  1: Train R² = 0.1144 | Test R² = -0.0868
Window  2: Train R² = 0.1675 | Test R² = 0.0400
Window  3: Train R² = 0.2252 | Test R² = -0.0810
Window  4: Train R² = 0.1458 | Test R² = -0.0576
Window  5: Train R² = 0.0800 | Test R² = 0.0274
Window  6: Train R² = 0.0842 | Test R² = -0.0274
Window  7: Train R² = 0.0905 | Test R² = -0.0157
Window  8: Train R² = 0.0617 | Test R² = -0.0169
Window  9: Train R² = 0.0694 | Test R² = -0.0619
Window 10: Train R² = 0.1069 | Test R² = -0.0180
Window 11: Train R² = 0.0674 | Test R² = -0.0938
Window 12: Train R² = 0.1242 | Test R² = -0.0936
Window 13: Train R² = 0.0776 | Test R² = -0.0928
Window 14: Train R² = 0.0325 | Test R² = -0.0011
Window 15: Train R² = 0.0257 | Test R² = -0.0096
Window 16: Train R² = 0.0615 | Test R² = 0.0219
Window 17: Train R² = 0.1037 | Test R² = -0.0187
Window 18: Train R² = 0.0705 | Test R² = -0.0768
Window 19: Train R² = 0.0333 | Test R² = -0.0163
Window 20: Train R² = 0.0814 | Test R² = -0.0164
Window 21: Train R² = 0.0689 | Test R² = -0.0124
Window 22: Train R² = 0.0714 | Test R² = -0.0344
Window 23: Train R² = 0.0766 | Test R² = -0.0383
Window 24: Train R² = 0.1022 | Test R² = -0.1749
Window 25: Train R² = 0.0633 | Test R² = -0.0574
Window 26: Train R² = 0.0987 | Test R² = -0.1918
Window 27: Train R² = 0.1063 | Test R² = -0.0048
Window 28: Train R² = 0.0933 | Test R² = -0.0483
Window 29: Train R² = 0.1066 | Test R² = -0.1008
Window 30: Train R² = 0.1421 | Test R² = -0.0688
Window 31: Train R² = 0.0686 | Test R² = -0.0212
Window 32: Train R² = 0.0809 | Test R² = -0.0627
Window 33: Train R² = 0.1197 | Test R² = -0.0787
Window 34: Train R² = 0.0979 | Test R² = -0.0512
Window 35: Train R² = 0.0969 | Test R² = -0.0398
Window 36: Train R² = 0.0988 | Test R² = -0.0489
Window 37: Train R² = 0.1153 | Test R² = -0.0559
Window 38: Train R² = 0.0800 | Test R² = -0.0695

CONFIGURATION:
Training years: 20
Validation years: 5
Test years: 1
Architecture: [32, 16, 8]
Ensemble size: 1
Min features: 20
Max features: 50
Hyperparameter tuning: True
